#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
ğŸ”¬ VPD å»å™ªæ•ˆæœéªŒè¯è„šæœ¬
=============================================================================

ç›®æ ‡ï¼šéªŒè¯ VPD æ–¹æ³•æ˜¯å¦"çœŸçš„åšæ‰äº†å™ªå£°æ–¹å‘"

éªŒè¯æŒ‡æ ‡ï¼š
1. åŸå§‹å‘é‡ Â· å™ªå£°æ–¹å‘ çš„åˆ†å¸ƒï¼ˆåº”è¯¥æœ‰æ˜æ˜¾æ–¹å‘æ€§ï¼‰
2. å»å™ªåå‘é‡ Â· å™ªå£°æ–¹å‘ çš„åˆ†å¸ƒï¼ˆåº”è¯¥æ¥è¿‘ 0ï¼‰
3. å™ªå£°åŸå‹å‘é‡çš„èŒƒæ•°ï¼ˆå¦‚æœå¤ªå°ï¼Œè¯´æ˜å™ªå£°ä¸ç¨³å®šï¼‰
4. æŠ•å½±å¼ºåº¦çš„å®é™…æ•ˆæœï¼ˆç†è®ºå€¼ vs å®é™…å€¼ï¼‰

ç§‘å­¦æ ‡å‡†ï¼š
- å¦‚æœ projection_strength=1.0 ä¸”åŸå‹å‘é‡ç¨³å®šï¼Œ
  å»å™ªååº”è¯¥å‡ ä¹å…¨éƒ½æ¥è¿‘ 0ï¼ˆæ•°å€¼è¯¯å·®çº§ < 1e-6ï¼‰
- å¦‚æœå»å™ªåä»æ˜æ˜¾ä¸ä¸º 0ï¼Œè¯´æ˜æœ‰ bugï¼ˆå½’ä¸€åŒ–/é¡ºåº/èåˆé—®é¢˜ï¼‰
"""

from __future__ import annotations

from pathlib import Path
import numpy as np
import json
import sys

# æ·»åŠ æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆä¾¿äºå¯¼å…¥ configï¼‰
sys.path.insert(0, str(Path(__file__).resolve().parent))

try:
    from config import PROJECT_PREFIX
except ImportError:
    PROJECT_PREFIX = "helicobacter_pylori"


def load_vectors_and_metadata():
    """åŠ è½½åŸå§‹å‘é‡å’Œå»å™ªå‘é‡"""
    root = Path(__file__).resolve().parent
    
    # è·¯å¾„ 1: åŸå§‹èåˆå‘é‡ï¼ˆStep 1ï¼‰
    fused_path = root / "05_stopwords" / "Experiment_C_Vector" / "data" / "c_step1_fused_vectors.npz"
    
    # è·¯å¾„ 2: å»å™ªåå‘é‡ï¼ˆStep 2ï¼‰
    clean_path = root / "05_stopwords" / "Experiment_C_Vector" / "data" / "c_step2_clean_vectors.npz"
    
    # è·¯å¾„ 3: æœ€ç»ˆè¾“å‡ºçš„å‘é‡ï¼ˆç”¨äº topic modelingï¼‰
    output_path = root / "06_denoised_data" / f"{PROJECT_PREFIX}_topic_modeling_VPD.csv"
    
    print("=" * 80)
    print("ğŸ“ è·¯å¾„æ£€æŸ¥")
    print("=" * 80)
    print(f"èåˆå‘é‡ (Step 1):  {fused_path.exists() and 'âœ“' or 'âœ—'} {fused_path}")
    print(f"æ¸…æ´å‘é‡ (Step 2):  {clean_path.exists() and 'âœ“' or 'âœ—'} {clean_path}")
    print(f"æœ€ç»ˆè¾“å‡º (CSV):     {output_path.exists() and 'âœ“' or 'âœ—'} {output_path}")
    
    if not fused_path.exists() or not clean_path.exists():
        raise FileNotFoundError("ç¼ºå°‘å…³é”®å‘é‡æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ Experiment C çš„ Step 1 å’Œ Step 2")
    
    # åŠ è½½å‘é‡
    print("\nåŠ è½½å‘é‡æ•°æ®...")
    fused_data = np.load(fused_path, allow_pickle=True)
    clean_data = np.load(clean_path, allow_pickle=True)
    
    fused_vectors = fused_data["fused_vectors"]  # (n_docs, 384)
    clean_vectors = clean_data["clean_vectors"]  # (n_docs, 384)
    noise_prototype = clean_data["noise_prototype"]  # (384,)
    noise_words = clean_data["noise_words"]
    projection_strength = clean_data["config_projection_strength"].item()
    
    print(f"  èåˆå‘é‡å½¢çŠ¶: {fused_vectors.shape}")
    print(f"  æ¸…æ´å‘é‡å½¢çŠ¶: {clean_vectors.shape}")
    print(f"  å™ªå£°åŸå‹ç»´åº¦: {noise_prototype.shape}")
    print(f"  å™ªå£°è¯æ•°é‡: {len(noise_words)}")
    print(f"  æŠ•å½±å¼ºåº¦é…ç½®: {projection_strength}")
    
    return {
        "fused_vectors": fused_vectors,
        "clean_vectors": clean_vectors,
        "noise_prototype": noise_prototype,
        "noise_words": noise_words,
        "projection_strength": projection_strength,
    }


def verify_noise_prototype_stability(noise_prototype):
    """éªŒè¯å™ªå£°åŸå‹å‘é‡çš„ç¨³å®šæ€§"""
    print("\n" + "=" * 80)
    print("ğŸ” éªŒè¯ 1: å™ªå£°åŸå‹å‘é‡çš„ç¨³å®šæ€§")
    print("=" * 80)
    
    norm = np.linalg.norm(noise_prototype)
    print(f"å™ªå£°åŸå‹å‘é‡çš„ L2 èŒƒæ•°: {norm:.6f}")
    
    if norm < 0.1:
        print("âš ï¸  WARNING: èŒƒæ•°è¿‡å° (<0.1)ï¼Œè¯´æ˜å™ªå£°æ–¹å‘ä¸ç¨³å®šï¼")
        print("   â†’ å¯èƒ½åŸå› : å™ªå£°è¯å®šä¹‰ä¸å¥½ï¼Œæˆ–æ•°æ®ä¸­æ²¡æœ‰è¿™äº›è¯çš„ä¸€è‡´æ–¹å‘")
        print("   â†’ å½±å“: æŠ•å½±å‡ºæ¥çš„æ–¹å‘ä¼šå¾ˆä¸ç¨³å®šï¼Œå»å™ªæ•ˆæœå·®")
        return False
    elif norm < 0.5:
        print("âš ï¸  CAUTION: èŒƒæ•°è¾ƒå° (0.1-0.5)ï¼Œå™ªå£°æ–¹å‘å¯èƒ½ä¸å¤Ÿæ¸…æ™°")
        return True
    else:
        print("âœ“ èŒƒæ•°åˆç† (>0.5)ï¼Œå™ªå£°æ–¹å‘å®šä¹‰æ¸…æ™°")
        return True
    
    # éªŒè¯æ˜¯å¦å·²å½’ä¸€åŒ–
    expected_norm = 1.0
    if abs(norm - expected_norm) < 1e-5:
        print("âœ“ å‘é‡å·²å•ä½åŒ– (||nÌ‚|| â‰ˆ 1.0)")
    else:
        print(f"âš ï¸  å‘é‡æœªå®Œå…¨å•ä½åŒ– (||nÌ‚|| = {norm:.6f} â‰  1.0)")


def verify_noise_similarity_distribution(fused_vectors, clean_vectors, noise_prototype):
    """éªŒè¯åŸå§‹å’Œå»å™ªå‘é‡ä¸å™ªå£°æ–¹å‘çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ"""
    print("\n" + "=" * 80)
    print("ğŸ” éªŒè¯ 2: ä¸å™ªå£°æ–¹å‘çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ")
    print("=" * 80)
    
    # è®¡ç®—ç‚¹ç§¯ï¼ˆç›¸ä¼¼åº¦ï¼‰
    fused_similarity = fused_vectors @ noise_prototype  # (n_docs,)
    clean_similarity = clean_vectors @ noise_prototype  # (n_docs,)
    
    print("\nã€åŸå§‹å‘é‡ä¸å™ªå£°æ–¹å‘çš„ç›¸ä¼¼åº¦ã€‘")
    print(f"  å‡å€¼:     {np.mean(fused_similarity):+.6f}")
    print(f"  ä¸­ä½æ•°:   {np.median(fused_similarity):+.6f}")
    print(f"  æ ‡å‡†å·®:   {np.std(fused_similarity):.6f}")
    print(f"  æœ€å°å€¼:   {np.min(fused_similarity):+.6f}")
    print(f"  æœ€å¤§å€¼:   {np.max(fused_similarity):+.6f}")
    print(f"  èŒƒå›´:     [{np.percentile(fused_similarity, 5):+.6f}, {np.percentile(fused_similarity, 95):+.6f}] (P5-P95)")
    
    print("\nã€å»å™ªå‘é‡ä¸å™ªå£°æ–¹å‘çš„ç›¸ä¼¼åº¦ã€‘")
    print(f"  å‡å€¼:     {np.mean(clean_similarity):+.6f}")
    print(f"  ä¸­ä½æ•°:   {np.median(clean_similarity):+.6f}")
    print(f"  æ ‡å‡†å·®:   {np.std(clean_similarity):.6f}")
    print(f"  æœ€å°å€¼:   {np.min(clean_similarity):+.6f}")
    print(f"  æœ€å¤§å€¼:   {np.max(clean_similarity):+.6f}")
    print(f"  èŒƒå›´:     [{np.percentile(clean_similarity, 5):+.6f}, {np.percentile(clean_similarity, 95):+.6f}] (P5-P95)")
    
    print("\nã€å»å™ªæ•ˆæœè¯„ä¼°ã€‘")
    mean_reduction = abs(np.mean(fused_similarity)) - abs(np.mean(clean_similarity))
    reduction_ratio = mean_reduction / abs(np.mean(fused_similarity)) if np.mean(fused_similarity) != 0 else 0
    print(f"  å¹³å‡ç›¸ä¼¼åº¦å‡å°‘: {mean_reduction:+.6f} ({reduction_ratio*100:.1f}%)")
    
    # éªŒè¯å…³é”®æŒ‡æ ‡ï¼šå»å™ªåæ˜¯å¦æ¥è¿‘ 0
    clean_mean_abs = np.mean(np.abs(clean_similarity))
    print(f"  |å»å™ªç›¸ä¼¼åº¦|çš„å¹³å‡å€¼: {clean_mean_abs:.6f}")
    
    if clean_mean_abs < 1e-5:
        print("âœ“ å®Œç¾ï¼å»å™ªåå‡ ä¹å®Œå…¨æ¶ˆé™¤å™ªå£°æ–¹å‘ï¼ˆ< 1e-5ï¼‰")
        return True
    elif clean_mean_abs < 1e-4:
        print("âœ“ å¾ˆå¥½ï¼å»å™ªåå™ªå£°æ–¹å‘éå¸¸å°ï¼ˆ< 1e-4ï¼‰")
        return True
    elif clean_mean_abs < 0.01:
        print("âš ï¸  ä¸€èˆ¬ã€‚å»å™ªåä»æœ‰ä¸€å®šå™ªå£°åˆ†é‡ï¼ˆ>1e-4ï¼‰")
        print("   â†’ å¯èƒ½åŸå› : å‘é‡éå•ä½å‘é‡ï¼Œæˆ–å½’ä¸€åŒ–æœ‰é—®é¢˜")
        return False
    else:
        print("âŒ ä¸è¡Œï¼å»å™ªæ•ˆæœå¾ˆå·®ï¼ˆ> 0.01ï¼‰")
        print("   â†’ è¿™è¡¨æ˜æŠ•å½±å¯èƒ½æ²¡æœ‰æ­£ç¡®æ‰§è¡Œ")
        return False


def verify_projection_reconstruction():
    """éªŒè¯æŠ•å½±çš„æ•°å­¦æ­£ç¡®æ€§"""
    print("\n" + "=" * 80)
    print("ğŸ” éªŒè¯ 3: æŠ•å½±æ•°å­¦çš„æ­£ç¡®æ€§ï¼ˆæ ·ä¾‹ï¼‰")
    print("=" * 80)
    
    root = Path(__file__).resolve().parent
    clean_path = root / "05_stopwords" / "Experiment_C_Vector" / "data" / "c_step2_clean_vectors.npz"
    
    clean_data = np.load(clean_path, allow_pickle=True)
    fused_path = root / "05_stopwords" / "Experiment_C_Vector" / "data" / "c_step1_fused_vectors.npz"
    fused_data = np.load(fused_path, allow_pickle=True)
    
    # ä» npz ä¸­æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹ç›¸ä¼¼åº¦å€¼
    original_noise_sim = float(clean_data.get("original_noise_similarity", 0.0))
    clean_noise_sim = float(clean_data.get("clean_noise_similarity", 0.0))
    
    print(f"\nä» Step 2 è¾“å‡ºä¸­è®°å½•çš„æ•°æ®ï¼š")
    print(f"  æŠ•å½±å‰å¹³å‡ç›¸ä¼¼åº¦: {original_noise_sim:+.6f}")
    print(f"  æŠ•å½±åå¹³å‡ç›¸ä¼¼åº¦: {clean_noise_sim:+.6f}")
    print(f"  ç›¸ä¼¼åº¦å‡å°‘ç™¾åˆ†æ¯”: {(1 - clean_noise_sim / original_noise_sim) * 100:.1f}%")
    
    # é‡æ–°è®¡ç®—éªŒè¯
    fused_vectors = fused_data["fused_vectors"]
    clean_vectors = clean_data["clean_vectors"]
    noise_prototype = clean_data["noise_prototype"]
    
    recomputed_fused = np.mean(fused_vectors @ noise_prototype)
    recomputed_clean = np.mean(clean_vectors @ noise_prototype)
    
    print(f"\né‡æ–°è®¡ç®—éªŒè¯ï¼š")
    print(f"  åŸå§‹ç›¸ä¼¼åº¦: {recomputed_fused:+.6f} (diff: {abs(original_noise_sim - recomputed_fused):.6e})")
    print(f"  æ¸…æ´ç›¸ä¼¼åº¦: {recomputed_clean:+.6f} (diff: {abs(clean_noise_sim - recomputed_clean):.6e})")
    
    if abs(recomputed_clean) < 1e-5:
        print("âœ“ æŠ•å½±æ•°å­¦æ­£ç¡®ï¼Œå™ªå£°æ–¹å‘å·²è¢«å®Œå…¨ç§»é™¤")
        return True
    else:
        print(f"âš ï¸  æŠ•å½±åä»æœ‰æ®‹ä½™ ({abs(recomputed_clean):.6e})")
        return False


def compare_with_baseline_embeddings():
    """ä¸ Baseline çš„åµŒå…¥è¿›è¡Œå¯¹æ¯”"""
    print("\n" + "=" * 80)
    print("ğŸ” éªŒè¯ 4: VPD å‘é‡ä¸ Baseline çš„å¯¹æ¯”")
    print("=" * 80)
    
    root = Path(__file__).resolve().parent
    clean_path = root / "06_denoised_data" / f"{PROJECT_PREFIX}_topic_modeling_VPD.csv"
    
    # VPD ç”¨çš„æ˜¯ c_final_clean_vectors.npz
    vpd_vec_path = root / "05_stopwords" / "Experiment_C_Vector" / "output" / "c_final_clean_vectors.npz"
    
    if not vpd_vec_path.exists():
        print(f"âš ï¸  æ‰¾ä¸åˆ° VPD å‘é‡æ–‡ä»¶: {vpd_vec_path}")
        print("   â†’ è¿™ä¸ªæ–‡ä»¶åº”è¯¥åœ¨ Step 3 ç”Ÿæˆï¼ˆ03_output_vectors.pyï¼‰")
        return None
    
    # åŠ è½½ VPD ç”¨çš„æœ€ç»ˆæ¸…æ´å‘é‡
    vpd_data = np.load(vpd_vec_path, allow_pickle=True)
    vpd_vectors = vpd_data["embeddings"]
    
    print(f"VPD æœ€ç»ˆå‘é‡å½¢çŠ¶: {vpd_vectors.shape}")
    print(f"å‘é‡å·²å½’ä¸€åŒ–: {np.allclose(np.linalg.norm(vpd_vectors, axis=1), 1.0)}")
    
    return vpd_vectors


def generate_report(results):
    """ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
    print("=" * 80)
    
    # æ±‡æ€»ç»“æœ
    verdict = all(results.values())
    
    if verdict:
        print("\nâœ… æ€»ä½“ç»“è®º: VPD å»å™ªå®ç°æ­£ç¡®ï¼")
        print("   â†’ å™ªå£°æ–¹å‘å·²è¢«æœ‰æ•ˆç§»é™¤")
        print("   â†’ å‘é‡ç©ºé—´å»å™ªçš„æ•°å­¦åŸç†å¾—åˆ°éªŒè¯")
        print("   â†’ VPD çš„ +5.2% C_v æå‡æ˜¯æœ‰åŸºç¡€çš„")
    else:
        print("\nâŒ æ€»ä½“ç»“è®º: VPD å»å™ªå­˜åœ¨é—®é¢˜ï¼")
        print("   â†’ ä»¥ä¸‹æ–¹é¢éœ€è¦æ£€æŸ¥:")
        if not results.get("noise_stability"):
            print("     - å™ªå£°åŸå‹å‘é‡ä¸å¤Ÿç¨³å®šï¼ˆèŒƒæ•°è¿‡å°ï¼‰")
        if not results.get("similarity_distribution"):
            print("     - å»å™ªåä»æœ‰æ˜æ˜¾å™ªå£°åˆ†é‡")
        if not results.get("projection_math"):
            print("     - æŠ•å½±è®¡ç®—å¯èƒ½æœ‰æ•°å€¼é—®é¢˜")
        print("\n   å»ºè®®:")
        print("     1. æ£€æŸ¥ noise_words çš„å®šä¹‰æ˜¯å¦åˆç†")
        print("     2. éªŒè¯å‘é‡çš„å•ä½åŒ–å¤„ç†")
        print("     3. è¿è¡Œ 05_stopwords/Experiment_C_Vector/ ä¸­çš„ Step 1-3")
    
    # è¾“å‡ºæ•°å€¼æ€»ç»“
    print("\nã€å…³é”®æ•°å€¼æ€»ç»“ã€‘")
    print(f"  â€¢ åŸå§‹å‘é‡å‡å€¼ç›¸ä¼¼åº¦: {results.get('original_mean', 'N/A')}")
    print(f"  â€¢ æ¸…æ´å‘é‡å‡å€¼ç›¸ä¼¼åº¦: {results.get('clean_mean', 'N/A')}")
    print(f"  â€¢ å™ªå£°å‡å°‘ç™¾åˆ†æ¯”: {results.get('reduction_ratio', 'N/A')}")
    print(f"  â€¢ æ¸…æ´å‘é‡|ç›¸ä¼¼åº¦|å‡å€¼: {results.get('clean_mean_abs', 'N/A')}")
    
    return verdict


def main():
    """ä¸»æµç¨‹"""
    results = {}
    
    try:
        # åŠ è½½æ•°æ®
        data = load_vectors_and_metadata()
        
        # éªŒè¯ 1: å™ªå£°åŸå‹ç¨³å®šæ€§
        results["noise_stability"] = verify_noise_prototype_stability(data["noise_prototype"])
        
        # éªŒè¯ 2: ç›¸ä¼¼åº¦åˆ†å¸ƒ
        fused_sim = data["fused_vectors"] @ data["noise_prototype"]
        clean_sim = data["clean_vectors"] @ data["noise_prototype"]
        results["original_mean"] = f"{np.mean(fused_sim):+.6f}"
        results["clean_mean"] = f"{np.mean(clean_sim):+.6f}"
        results["clean_mean_abs"] = f"{np.mean(np.abs(clean_sim)):.6e}"
        results["reduction_ratio"] = f"{(1 - np.mean(clean_sim) / np.mean(fused_sim)) * 100:.1f}%"
        results["similarity_distribution"] = verify_noise_similarity_distribution(
            data["fused_vectors"], 
            data["clean_vectors"], 
            data["noise_prototype"]
        )
        
        # éªŒè¯ 3: æŠ•å½±æ•°å­¦æ­£ç¡®æ€§
        results["projection_math"] = verify_projection_reconstruction()
        
        # éªŒè¯ 4: ä¸ Baseline å¯¹æ¯”
        compare_with_baseline_embeddings()
        
        # ç”ŸæˆæŠ¥å‘Š
        verdict = generate_report(results)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = Path(__file__).resolve().parent / "verification_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\néªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return 0 if verdict else 1
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
