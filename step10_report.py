#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
step10_report.py
Step 10ï¼šç”Ÿæˆâ€œä¸¥æ ¼å¯å¤ç°â€çš„ç ”ç©¶æŠ¥å‘Šï¼ˆæŒ‰æ–¹æ³•åˆ†åˆ«è¾“å‡ºï¼‰

ä½ æçš„å…³é”®è¦æ±‚ï¼Œæœ¬è„šæœ¬å®ç°ï¼š
- æ¯ä¸ªæ–¹æ³•ï¼ˆbaseline/A/B/C/AB/ABCï¼‰æŠ¥å‘Šå†…å®¹ä¸åŒï¼ˆå› ä¸º stopwords/åˆ è¯/æ¨¡å‹ç»“æœä¸åŒï¼‰
- æŠ¥å‘Šå¿…é¡»ä½“ç° Step05/06/07ï¼šåšäº†ä»€ä¹ˆã€è·‘äº†å“ªäº›å­æ­¥éª¤ã€ç”Ÿæˆäº†å¤šå°‘ã€è¾“å‡ºæ–‡ä»¶åœ¨å“ªé‡Œã€ç¤ºä¾‹æœ‰å“ªäº›
- åŒæ—¶æŠŠå…³é”®ç»Ÿè®¡åœ¨ç»ˆç«¯ä¹Ÿæ‰“å°ä¸€éï¼ˆä¸ä¼šåªå†™åœ¨ report é‡Œï¼‰
- Step08 çš„ best mc ä¸€è‡´æ€§é€‰æ‹©ï¼Œä¼šè¢«æŠ¥å‘Šå¼•ç”¨ï¼Œå¹¶ç”¨äºå®šä½ Step07 çš„â€œæœ€ç»ˆé‡‡ç”¨ç‰ˆæœ¬â€

è¾“å…¥ï¼š
- 04_filtered_data/*_filter_log.txtï¼ˆå¯é€‰ï¼‰
- 05_stopwords/stopwords_manifest.json
- 06_denoised_data/denoise_manifest.json
- 07_topic_models/topic_models_manifest.jsonï¼ˆå¯é€‰ï¼‰
- 08_model_selection/best_mc_by_method.json
- 07_topic_models/<METHOD>/{PROJECT_PREFIX}_mc{best}_*.csv
- 09_visualization/<METHOD>/*.pngï¼ˆå¯é€‰ï¼‰

è¾“å‡ºï¼š
- 10_report/<METHOD>/{PROJECT_PREFIX}_{method}_report.md

ç”¨æ³•ï¼š
- python step10_report.py
- python step10_report.py --only ABC
"""

from __future__ import annotations

import argparse
import json
import time
import re
import subprocess
import sys
import platform
import hashlib
import locale
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

import pandas as pd

# HTML è½¬æ¢æ”¯æŒ
try:
    import markdown
    HAS_MARKDOWN = True
except ImportError:
    HAS_MARKDOWN = False

# DOCX è½¬æ¢æ”¯æŒï¼ˆå¯é€‰ï¼‰
try:
    import pypandoc  # type: ignore
    HAS_PYPANDOC = True
except ImportError:
    HAS_PYPANDOC = False

try:
    from config import PROJECT_PREFIX, SEARCH_KEYWORD, get_project_name
except ImportError:
    print("è¯·ç¡®ä¿ config.py å­˜åœ¨ä¸”é…ç½®æ­£ç¡®")
    raise


ALL_METHODS = ["baseline", "VPD"]  # å·²åœç”¨ A/B/AB/ABCï¼Œå½’æ¡£äº 07_topic_models/_archived_AB_methods


def _safe_load_json(path: Path) -> Optional[Dict[str, Any]]:
    if not path.exists():
        return None
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def _read_filter_log(base_dir: Path) -> Optional[str]:
    logs = list((base_dir / "04_filtered_data").glob(f"{PROJECT_PREFIX}_filter_log.txt"))
    if not logs:
        return None
    try:
        return logs[0].read_text(encoding="utf-8", errors="replace")
    except Exception:
        return None


def _pick_best_files(base_dir: Path, method: str, best_mc: int) -> Dict[str, Path]:
    d = base_dir / "07_topic_models" / method.upper()
    return {
        "topic_info": d / f"{PROJECT_PREFIX}_mc{best_mc}_topic_info.csv",
        "frontier": d / f"{PROJECT_PREFIX}_mc{best_mc}_frontier_indicators.csv",
        "doc_map": d / f"{PROJECT_PREFIX}_mc{best_mc}_doc_topic_mapping.csv",
        "summary": d / f"{PROJECT_PREFIX}_mc{best_mc}_summary.xlsx",
    }


def _load_step08_context(base_dir: Path) -> Dict[str, Any]:
    """Load Step08 selection artifacts for reporting."""
    out: Dict[str, Any] = {"best": {}, "scores_full": {}}
    best_path = base_dir / "08_model_selection" / "best_mc_by_method.json"
    scores_path = base_dir / "08_model_selection" / "cv_scores_full.json"
    best = _safe_load_json(best_path) or {}
    scores = _safe_load_json(scores_path) or {}
    out["best"] = best
    out["scores_full"] = scores
    out["best_mtime"] = best_path.stat().st_mtime if best_path.exists() else 0.0
    out["scores_mtime"] = scores_path.stat().st_mtime if scores_path.exists() else 0.0
    return out


def _get_pkg_version(name: str) -> str:
    try:
        from importlib.metadata import version  # type: ignore

        return version(name)
    except Exception:
        return "-"


def _get_git_commit_hash(base_dir: Path) -> str:
    """Best-effort git commit hash for reproducibility."""
    try:
        out = subprocess.check_output(["git", "-C", str(base_dir), "rev-parse", "HEAD"], stderr=subprocess.DEVNULL)
        s = out.decode("utf-8", errors="replace").strip()
        return s if s else "-"
    except Exception:
        return "-"


def _sha256_file(path: Path) -> str:
    try:
        h = hashlib.sha256()
        with path.open("rb") as f:
            for chunk in iter(lambda: f.read(1024 * 1024), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return "-"


def _fmt_mtime(ts: float) -> str:
    try:
        return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return "-"


def _count_csv_rows_fast(path: Path) -> Optional[int]:
    """Count CSV rows quickly without pandas; returns number of data rows (excluding header)."""
    try:
        with path.open("rb") as f:
            n = 0
            for _ in f:
                n += 1
        return max(0, n - 1)
    except Exception:
        return None


def _step07_review_manifest(base_dir: Path, method: str) -> Optional[Dict[str, Any]]:
    p = base_dir / "07_topic_models" / method.upper() / "review_manifest.json"
    return _safe_load_json(p)


def _fingerprint_row(path: Path, *, want_rows: bool = True) -> str:
    if not path.exists():
        return f"| {path.as_posix()} | - | - | - | - |"
    st = path.stat()
    size_kb = st.st_size / 1024.0
    mtime = _fmt_mtime(st.st_mtime)
    rows = "-"
    if want_rows and path.suffix.lower() == ".csv":
        r = _count_csv_rows_fast(path)
        rows = str(r) if isinstance(r, int) else "-"
    sha = _sha256_file(path)
    sha12 = sha[:12] if isinstance(sha, str) and len(sha) >= 12 else sha
    return f"| {path.as_posix()} | {rows} | {mtime} | {size_kb:.1f} KB | {sha12} |"


def _reproducibility_section(base_dir: Path, method: str, *, best_mc: int) -> List[str]:
    """A compact reproducibility checklist for top-journal style reporting."""
    lines: List[str] = []
    lines.append("### å¤ç°æ€§æ¸…å•ï¼ˆReproducibility Checklistï¼‰\n")
    lines.append("| é¡¹ç›® | å€¼ |")
    lines.append("|---|---|")
    lines.append(f"| OS / Platform | {platform.platform()} |")
    lines.append(f"| Python | {platform.python_version()} |")

    # Code identity
    git_hash = _get_git_commit_hash(base_dir)
    lines.append(f"| Git commit hash | {git_hash} |")
    rm = _step07_review_manifest(base_dir, method) or {}
    code_version = rm.get("code_version") if isinstance(rm, dict) else None
    lines.append(f"| Step07 code_version | {code_version or '-'} |")

    # Seeds & sampling
    lines.append("| Step07 adaptive mc density sampling seed | 42 (å›ºå®šï¼›è§ _engine_bertopic.py) |")
    lines.append("| Step08 text sampling seed | 20251220ï¼ˆé»˜è®¤ï¼Œå¯ç”¨ step08_cv_select.py --seed ä¿®æ”¹ï¼‰ |")
    lines.append("| Step08 max_docs | 3000ï¼ˆé»˜è®¤ï¼Œå¯ç”¨ step08_cv_select.py --max_docs ä¿®æ”¹ï¼›0=å…¨é‡ï¼‰ |")

    # Key packages (best-effort)
    lines.append(f"| pandas | {_get_pkg_version('pandas')} |")
    lines.append(f"| numpy | {_get_pkg_version('numpy')} |")
    lines.append(f"| gensim | {_get_pkg_version('gensim')} |")
    lines.append(f"| bertopic | {_get_pkg_version('bertopic')} |")
    lines.append(f"| hdbscan | {_get_pkg_version('hdbscan')} |")
    lines.append(f"| umap-learn | {_get_pkg_version('umap-learn')} |")
    lines.append(f"| matplotlib | {_get_pkg_version('matplotlib')} |")
    lines.append(f"| seaborn | {_get_pkg_version('seaborn')} |")
    lines.append(f"| pypandoc (optional) | {_get_pkg_version('pypandoc')} |")
    lines.append("")

    # Data fingerprints
    lines.append("### è¾“å…¥æ•°æ®æŒ‡çº¹ï¼ˆInput Fingerprintsï¼‰\n")
    lines.append("è¯´æ˜ï¼šç”¨äºå®¡ç¨¿å¤ç°å¯¹é½ã€‚sha256 å–å‰ 12 ä½ï¼›rows ä¸º CSV æ•°æ®è¡Œæ•°ï¼ˆä¸å«è¡¨å¤´ï¼‰ã€‚\n")
    lines.append("| æ–‡ä»¶ | rows | mtime | size | sha256[:12] |")
    lines.append("|---|---:|---|---:|---|")

    # Step08 artifacts
    lines.append(_fingerprint_row(base_dir / "08_model_selection" / "best_mc_by_method.json", want_rows=False))
    lines.append(_fingerprint_row(base_dir / "08_model_selection" / "cv_scores_full.json", want_rows=False))

    # Step07 chosen outputs
    d = base_dir / "07_topic_models" / method.upper()
    lines.append(_fingerprint_row(d / f"{PROJECT_PREFIX}_mc{best_mc}_topic_info.csv"))
    lines.append(_fingerprint_row(d / f"{PROJECT_PREFIX}_mc{best_mc}_frontier_indicators.csv"))
    lines.append(_fingerprint_row(d / f"{PROJECT_PREFIX}_mc{best_mc}_doc_topic_mapping.csv"))
    lines.append(_fingerprint_row(d / "review_manifest.json", want_rows=False))

    # Step06 input to Step07
    lines.append(_fingerprint_row(base_dir / "06_denoised_data" / f"{PROJECT_PREFIX}_topic_modeling_{method}.csv"))

    lines.append("")
    lines.append("å¤‡æ³¨ï¼šè‹¥å¤ç°æ—¶å›¾ä¸­æ–‡å­—å‡ºç°æ–¹å—ï¼Œè¯·ç¡®è®¤ç³»ç»Ÿå­˜åœ¨å¯ç”¨ä¸­æ–‡å­—ä½“ï¼ˆå¦‚ Microsoft YaHei/SimHeiï¼‰ï¼Œæˆ–åœ¨ Step09 ä¸­æ£€æŸ¥å­—ä½“è‡ªåŠ¨é€‰æ‹©æ—¥å¿—ã€‚\n")
    return lines


def _maybe_refresh_step09(base_dir: Path, method: str, *, reference_mtime: float, force: bool = False) -> None:
    """Ensure Step09 outputs are present and not older than Step08 selection.

    This prevents Step10 embedding stale figures after Step08 changes.
    """
    out_dir = base_dir / "09_visualization" / method.upper()
    needs = force or (not out_dir.exists())

    key_files = [
        out_dir / "fig02_frontier_evolution.png",
        out_dir / "fig06_frontier_bubble.png",
        out_dir / "fig07_temporal_evolution.png",
        out_dir / "viz_report.html",
    ]

    if not needs:
        for p in key_files:
            if not p.exists():
                needs = True
                break
            try:
                if p.stat().st_mtime + 1e-6 < reference_mtime:
                    needs = True
                    break
            except Exception:
                needs = True
                break

    if not needs:
        return

    try:
        print(f"  [Step09] æ£€æµ‹åˆ°å¯è§†åŒ–ç¼ºå¤±/è¿‡æœŸï¼Œè‡ªåŠ¨é‡è·‘: {method}")
        script = base_dir / "step09_visualization.py"
        subprocess.check_call([sys.executable, str(script), "--only", method], cwd=str(base_dir))
    except Exception as e:
        print(f"  [Step09] è‡ªåŠ¨é‡è·‘å¤±è´¥ï¼Œå°†ç»§ç»­ç”ŸæˆæŠ¥å‘Šï¼ˆå¯èƒ½åµŒå…¥æ—§å›¾/ç¼ºå›¾ï¼‰ï¼š{str(e)[:160]}")


def _method_stopword_section(method: str, sw_manifest: Optional[Dict[str, Any]]) -> List[str]:
    lines: List[str] = []
    lines.append("## Step 05 åœç”¨è¯/å‘é‡äº§ç‰©ï¼ˆA/B/Cï¼‰\n")

    if sw_manifest is None:
        lines.append("- æœªæ‰¾åˆ° stopwords_manifest.jsonï¼ˆè¯·å…ˆè¿è¡Œ step05_stopwords.pyï¼‰\n")
        return lines

    schemes = sw_manifest.get("schemes", {})

    def add_scheme(letter: str, title: str):
        info = schemes.get(letter)
        if not info:
            lines.append(f"- æ–¹æ¡ˆ{letter}ï¼šæœªç”Ÿæˆ/ç¼ºå¤±\n")
            return
        ok = "âœ…" if info.get("ok") else "âŒ"
        lines.append(f"- æ–¹æ¡ˆ{letter}ï¼ˆ{title}ï¼‰ï¼š{ok}")
        # steps
        steps = info.get("steps", [])
        if steps:
            lines.append(f"  - å­æ­¥éª¤æ•°: {len(steps)}")
            for s in steps:
                lines.append(f"    - {s.get('name')} | rc={s.get('returncode')} | {s.get('seconds',0):.1f}s")
        # artifacts preview
        artifacts = info.get("artifacts", {})
        for k, v in artifacts.items():
            if isinstance(v, dict) and v.get("exists"):
                if "count" in v:
                    lines.append(f"  - {k}: {v.get('count')} è¯ | {Path(v.get('path','')).as_posix()}")
                    sample = v.get("sample") or []
                    if sample:
                        lines.append("    - ç¤ºä¾‹(å‰25): " + ", ".join(sample[:25]))
                else:
                    # npz
                    lines.append(f"  - {k}: {v.get('bytes',0)/(1024*1024):.1f} MB | {Path(v.get('path','')).as_posix()}")
            elif isinstance(v, dict):
                lines.append(f"  - {k}: ç¼ºå¤± | {Path(v.get('path','')).as_posix()}")

        lines.append("")

    # baselineï¼šä»ç„¶æ±‡æŠ¥ Step05ï¼Œä½†è¯´æ˜ä¸ä½¿ç”¨
    if method == "baseline":
        lines.append("- baselineï¼šæœ¬æ–¹æ³•ä¸ä½¿ç”¨åœç”¨è¯åˆ è¯ï¼›ä»…ç”¨äºå¯¹ç…§ã€‚\n")
        return lines

    # A / AB / ABC
    if "A" in method:
        add_scheme("A", "ç»Ÿè®¡å»å™ªï¼ˆSIDâ†’EVTâ†’Dynamic IDFâ†’Mergerï¼‰")
    # B / AB / ABC
    if "B" in method:
        add_scheme("B", "è¯­ä¹‰æ‰©å±•ï¼ˆSPAâ†’CNIâ†’SECï¼‰")
    # C / ABC
    if "C" in method:
        add_scheme("C", "å‘é‡æŠ•å½±ï¼ˆV-Fusionâ†’RepEâ†’Output Vectorsï¼‰")

    return lines


def _method_denoise_section(method: str, denoise_manifest: Optional[Dict[str, Any]]) -> List[str]:
    lines: List[str] = []
    lines.append("## Step 06 æ–‡æœ¬å»å™ªç»Ÿè®¡ï¼ˆæŒ‰æ–¹æ³•ï¼‰\n")

    if denoise_manifest is None:
        lines.append("- æœªæ‰¾åˆ° denoise_manifest.jsonï¼ˆè¯·å…ˆè¿è¡Œ step06_denoise.pyï¼‰\n")
        return lines

    m = denoise_manifest.get("methods", {}).get(method)
    if not m:
        lines.append(f"- æœªæ‰¾åˆ°æ–¹æ³• {method} çš„å»å™ªç»Ÿè®¡\n")
        return lines

    lines.append(f"- è¾“å‡º: {m.get('output_file')}")
    lines.append(f"- è¡Œæ•°: {m.get('rows_in')} â†’ {m.get('rows_out')}")
    lines.append(f"- token: {m.get('total_tokens_before')} â†’ {m.get('total_tokens_after')}ï¼ˆåˆ é™¤ {m.get('tokens_removed')}ï¼‰")

    if method in ("baseline", "C"):
        lines.append("- è¯´æ˜: æœ¬æ–¹æ³•ä¸åšåˆ è¯ï¼ˆbaseline å¯¹ç…§ / C çš„è´¡çŒ®åœ¨ Step05 å‘é‡æŠ•å½±ä½“ç°ï¼‰\n")
        return lines

    lines.append(f"- åœç”¨è¯: loaded={m.get('stopwords_loaded')} protected={m.get('protected_words')} effective={m.get('stopwords_effective')}")
    sw_sample = m.get("stopwords_sample") or []
    if sw_sample:
        lines.append("- åœç”¨è¯ç¤ºä¾‹(å‰25): " + ", ".join(sw_sample[:25]))

    removed_top = m.get("removed_top") or []
    if removed_top:
        top10 = removed_top[:10]
        lines.append("- åˆ é™¤token Top10: " + ", ".join([f"{w}({c})" for w, c in top10]))

    lines.append("")
    return lines


def _method_topic_model_section(
    base_dir: Path,
    method: str,
    best_mc: int,
    topic_phrase_map: Optional[Dict[int, str]] = None,
) -> List[str]:
    lines: List[str] = []
    lines.append("## Step 07 ä¸»é¢˜å»ºæ¨¡ä¸å‰æ²¿è¯†åˆ«ï¼ˆæœ€ç»ˆé‡‡ç”¨ best mc ç‰ˆæœ¬ï¼‰\n")

    files = _pick_best_files(base_dir, method, best_mc)
    ti = files["topic_info"]
    dm = files["doc_map"]
    fi = files["frontier"]

    lines.append(f"- best_mc = {best_mc}ï¼ˆæ¥è‡ª Step08 C_v é€‰æ‹©ï¼‰")
    lines.append(f"- topic_info: {ti.as_posix()}")
    lines.append(f"- frontier_indicators: {fi.as_posix()}")
    lines.append(f"- doc_topic_mapping: {dm.as_posix()}\n")

    if ti.exists():
        dft = pd.read_csv(ti)
        if "Topic" in dft.columns:
            num_topics = int((dft["Topic"] >= 0).sum())
            lines.append(f"- æœ‰æ•ˆä¸»é¢˜æ•°: {num_topics}")
        if "Count" in dft.columns:
            if "Topic" in dft.columns:
                dft2 = dft[dft["Topic"] >= 0].copy()
            else:
                dft2 = dft.copy()
            top = dft2.sort_values("Count", ascending=False).head(5)
            lines.append("- Top5ä¸»é¢˜(æŒ‰Count):")
            col = "TopWords" if "TopWords" in dft.columns else ("Representation" if "Representation" in dft.columns else None)
            for _, r in top.iterrows():
                try:
                    tid = int(r.get("Topic"))
                except Exception:
                    tid = None

                words = str(r.get(col, "")) if col else ""
                words = words.replace("\n", " ")

                if tid is not None and topic_phrase_map and tid in topic_phrase_map:
                    label = topic_phrase_map[tid]
                else:
                    if col == "TopWords":
                        kws = _extract_keywords_from_topwords(words, limit=10)
                    elif col == "Representation":
                        kws = _extract_keywords_from_representation(words, limit=10)
                    else:
                        kws = []
                    label = _generate_topic_phrase(kws)

                lines.append(f"  - {label}: Count={r.get('Count')} | {words[:120]}")

    if dm.exists():
        dfm = pd.read_csv(dm)
        if "Topic" in dfm.columns:
            noise = int((dfm["Topic"] == -1).sum())
            lines.append(f"- æ–‡çŒ®æ•°: {len(dfm)} | å™ªå£°æ–‡çŒ®: {noise} | å™ªå£°æ¯”ä¾‹: {noise/max(1,len(dfm)):.2%}")

    lines.append("")
    return lines


def _method_viz_section(base_dir: Path, method: str) -> List[str]:
    lines: List[str] = []
    lines.append("## Step 09 å¯è§†åŒ–äº§ç‰©\n")
    d = base_dir / "09_visualization" / method.upper()
    if not d.exists():
        lines.append("- æœªæ‰¾åˆ°å¯è§†åŒ–ç›®å½•ï¼ˆå¯å…ˆè¿è¡Œ step09_visualization.pyï¼‰\n")
        return lines
    imgs = sorted(d.glob("*.png"))
    lines.append(f"- å›¾è¡¨æ•°é‡: {len(imgs)}")

    viz_report = d / "viz_report.html"
    if viz_report.exists():
        rel_report = Path("../../") / "09_visualization" / method.upper() / viz_report.name
        lines.append(f"- è§£é‡Šé¡µ: <a href=\"{rel_report.as_posix()}\">viz_report.html</a>")
    for img in imgs[:20]:
        # ç›¸å¯¹è·¯å¾„ï¼šHTML åœ¨ 10_report/<METHOD>/ ä¸‹ï¼Œéœ€è¦å›åˆ°æ ¹ç›®å½•å†è¿› 09_visualization
        rel = Path("../../") / "09_visualization" / method.upper() / img.name
        lines.append(f"  - ![{img.stem}]({rel.as_posix()})")
    lines.append("")
    return lines


def _citations_section() -> List[str]:
    return [
        "## å‚è€ƒæ–‡çŒ®\n",
        "- Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv:2203.05794.",
        "- McInnes, L., Healy, J., & Astels, S. (2017). hdbscan: Hierarchical density based clustering. JOSS, 2(11), 205.",
        "- McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform Manifold Approximation and Projection. arXiv:1802.03426.",
        "- Campello, R. J., Moulavi, D., & Sander, J. (2013). Density-based clustering based on hierarchical density estimates. PAKDD.",
        "- RÃ¶der, M., Both, A., & Hinneburg, A. (2015). Exploring the space of topic coherence measures. WSDM.",
        "- Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation of topic coherence. NAACL.",
        "- Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal.",
        "- Small, H., Boyack, K. W., & Klavans, R. (2014). Identifying emerging topics in science and technology. Research Policy, 43(8), 1450-1467.",
        "- Chen, C. (2006). CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature. JASIST, 57(3), 359-377.",
        "- Diakoulaki, D., Mavrotas, G., & Papayannakis, L. (1995). Determining objective weights in multiple criteria problems: The CRITIC method. Computers & Operations Research, 22(7), 763-770.",
        "- Zou, A., et al. (2023). Representation Engineering. arXiv:2310.01405.",
        "",
    ]


def _methodology_section(*, base_dir: Path, method: str, best_mc: int) -> List[str]:
    """ç”Ÿæˆç ”ç©¶æ–¹æ³•è®ºç« èŠ‚ï¼ŒåŒ…å«å…¬å¼å’Œç†è®ºä¾æ®"""
    return [
        "## ç ”ç©¶æ–¹æ³•\n",
        "### æŠ€æœ¯è·¯çº¿\n",
        "æœ¬ç ”ç©¶é‡‡ç”¨ä»¥ä¸‹æµç¨‹è¿›è¡Œä¸»é¢˜å»ºæ¨¡ä¸ç ”ç©¶å‰æ²¿è¯†åˆ«ï¼š\n",
        "```",
        "Step 1: æ•°æ®é‡‡é›† (PubMed API) â†’ Step 2: å¼•ç”¨æ•°æ®è¡¥å…… (OpenCitations)",
        "    â†“",
        "Step 3: æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç† â†’ Step 4: æ–‡çŒ®ç±»å‹è¿‡æ»¤",
        "    â†“",
        "Step 5: æ–‡æœ¬å»å™ª (åœç”¨è¯ + ä¿æŠ¤è¯) â†’ Step 6: å»å™ªæ•°æ®åº”ç”¨",
        "    â†“",
        "Step 7: BERTopic ä¸»é¢˜å»ºæ¨¡ â†’ Step 8: C_v ä¸€è‡´æ€§è¯„ä¼°",
        "    â†“",
        "Step 9: å¯è§†åŒ–åˆ†æ â†’ Step 10: æŠ¥å‘Šç”Ÿæˆ",
        "```\n",
        "### æ ¸å¿ƒç®—æ³•\n",
        "#### BERTopic ä¸»é¢˜å»ºæ¨¡\n",
        "BERTopic æ˜¯ä¸€ç§åŸºäº Transformer çš„ç¥ç»ä¸»é¢˜å»ºæ¨¡æ–¹æ³• (Grootendorst, 2022)ï¼Œæ ¸å¿ƒæµç¨‹å¦‚ä¸‹ï¼š\n",
        "**Step 1: æ–‡æ¡£åµŒå…¥** - ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å°†æ–‡æ¡£è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡ï¼š",
        "$$\\mathbf{e}_i = \\text{SentenceTransformer}(d_i), \\quad \\mathbf{e}_i \\in \\mathbb{R}^{384}$$\n",
        "**Step 2: é™ç»´ (UMAP)** - ä½¿ç”¨ UMAP è¿›è¡Œéçº¿æ€§é™ç»´ (McInnes et al., 2018)ï¼š",
        "$$\\mathbf{u}_i = \\text{UMAP}(\\mathbf{e}_i; n_{\\text{neighbors}}=15, n_{\\text{components}}=5)$$\n",
        "**Step 3: èšç±» (HDBSCAN)** - ä½¿ç”¨ HDBSCAN è¿›è¡Œå¯†åº¦èšç±» (McInnes et al., 2017)ï¼š",
        "$$c_i = \\text{HDBSCAN}(\\mathbf{u}_i; \\text{min\\_cluster\\_size})$$",
        "å…¶ä¸­ $c_i \\in \\{-1, 0, 1, ..., K\\}$ï¼Œ$c_i = -1$ è¡¨ç¤ºå™ªå£°ç‚¹ã€‚\n",
        "**Step 4: ä¸»é¢˜è¡¨ç¤º (c-TF-IDF)** - åŸºäºç±»åˆ«çš„ TF-IDF æå–ä¸»é¢˜å…³é”®è¯ï¼š",
        "$$\\text{c-TF-IDF}_{t,c} = \\frac{f_{t,c}}{\\sum_{t'} f_{t',c}} \\cdot \\log\\left(1 + \\frac{A}{f_t}\\right)$$",
        "å…¶ä¸­ $f_{t,c}$ ä¸ºè¯ $t$ åœ¨ä¸»é¢˜ $c$ ä¸­çš„é¢‘ç‡ï¼Œ$A$ ä¸ºæ–‡æ¡£æ€»æ•°ã€‚\n",
        "#### CRITIC å®¢è§‚èµ‹æƒæ³•\n",
        "CRITIC (Diakoulaki et al., 1995) æ˜¯ä¸€ç§åŸºäºæŒ‡æ ‡å¯¹æ¯”å¼ºåº¦å’Œå†²çªæ€§çš„å®¢è§‚èµ‹æƒæ–¹æ³•ï¼š\n",
        "**Step 1: æ•°æ®æ ‡å‡†åŒ–**",
        "$$x_{ij}^* = \\frac{x_{ij} - \\min_j(x_{ij})}{\\max_j(x_{ij}) - \\min_j(x_{ij})}$$\n",
        "**Step 2: è®¡ç®—ä¿¡æ¯é‡**",
        "$$C_j = \\sigma_j \\cdot \\sum_{k \\neq j} (1 - r_{jk})$$",
        "å…¶ä¸­ $\\sigma_j$ ä¸ºæŒ‡æ ‡æ ‡å‡†å·®ï¼Œ$r_{jk}$ ä¸ºæŒ‡æ ‡é—´ç›¸å…³ç³»æ•°ã€‚\n",
        "**Step 3: è®¡ç®—æƒé‡**",
        "$$w_j = \\frac{C_j}{\\sum_{k=1}^m C_k}$$\n",
        "#### ç ”ç©¶å‰æ²¿ç»¼åˆæŒ‡æ•°\n",
        "$$\\text{Composite Index}_i = \\sum_{j=1}^{5} w_j \\cdot x_{ij}^*$$",
        "å…¶ä¸­äº”ä¸ªè¯„ä»·æŒ‡æ ‡ä¸ºï¼šStrengthï¼ˆè§„æ¨¡ï¼‰ã€Noveltyï¼ˆæ–°é¢–æ€§ï¼‰ã€Heatï¼ˆçƒ­åº¦ï¼‰ã€Avg_Citationsï¼ˆå¼•ç”¨åº¦ï¼‰ã€HighCited_Countï¼ˆé«˜è¢«å¼•ï¼‰ã€‚\n",

        "#### Step08ï¼šmin_cluster_size (mc) çš„å¤šç›®æ ‡é€‰æ‹©ï¼ˆç¡¬çº¦æŸ + Paretoï¼‰\n",
        "ä¸ºä¿è¯å¯å¤ç°ä¸å®¡ç¨¿å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬å°† mc é€‰æ‹©è¡¨è¿°ä¸ºæ ‡å‡†å¤šç›®æ ‡é—®é¢˜ã€‚å¯¹å€™é€‰ $mc_i$ å®šä¹‰ï¼š",
        "$$c_i = C_v(mc_i) \\quad (\\text{maximize}), \\qquad n_i = noise(mc_i) \\quad (\\text{minimize})$$",
        "å…¶ä¸­ $noise$ ä¸º doc_topic_mapping ä¸­ $Topic=-1$ çš„æ–‡æ¡£å æ¯”ã€‚å¯¹é baseline æ–¹æ³•ï¼Œå¼•å…¥æ¥è‡ªçˆ¶æ–¹æ³•çš„å™ªéŸ³çº¦æŸé˜ˆå€¼ $r$ï¼š",
        "$$F(r)=\\{i\\mid n_i \\le r\\}$$",
        "Paretoï¼ˆå¸•ç´¯æ‰˜ï¼‰æ”¯é…ï¼šå€™é€‰ $a$ æ”¯é… $b$ å½“ä¸”ä»…å½“ $(c_a\\ge c_b) \\wedge (n_a\\le n_b)$ ä¸”è‡³å°‘ä¸€é¡¹ä¸¥æ ¼æ›´ä¼˜ï¼›å¸•ç´¯æ‰˜å‰æ²¿ä¸ºæ‰€æœ‰ä¸è¢«æ”¯é…çš„å€™é€‰é›†åˆã€‚",
        "å†³ç­–è§„åˆ™ï¼š",
        "- è‹¥ $F(r)$ éç©ºï¼šåœ¨ $Pareto(F)$ ä¸Šé€‰æ‹© $c$ æœ€å¤§è€…ï¼›è‹¥å¹¶åˆ—é€‰ $n$ æ›´å°è€…ï¼ˆå†å¹¶åˆ—é€‰æ›´å°çš„ $mc$ï¼Œç¡®ä¿ç¡®å®šæ€§ï¼‰ã€‚",
        "- è‹¥ $F(r)$ ä¸ºç©ºï¼šè¯´æ˜æ²¡æœ‰å€™é€‰æ»¡è¶³å™ªéŸ³é˜ˆå€¼ï¼›åœ¨ $Pareto(All)$ ä¸Šä¼˜å…ˆé€‰æ‹© $n$ æœ€å°è€…ï¼›è‹¥å¹¶åˆ—é€‰æ‹© $c$ æ›´å¤§è€…ï¼ˆå†å¹¶åˆ—é€‰æ›´å°çš„ $mc$ï¼‰ã€‚\n",
        "è¯¥ç­–ç•¥é¿å…ç”¨æ›´å·®çš„å™ªéŸ³æ¢å–è¡¨è§‚æ›´é«˜çš„ä¸€è‡´æ€§åˆ†æ•°ï¼Œå¹¶åœ¨ä¸å¯è¡Œæ—¶ç»™å‡ºä¿å®ˆã€å¯è§£é‡Šçš„é€€åŒ–è·¯å¾„ï¼ˆRÃ¶der et al., 2015; Newman et al., 2010ï¼‰ã€‚\n",

        "#### Step07ï¼šè‡ªé€‚åº”å€™é€‰ mc çš„ç”Ÿæˆï¼ˆå…¬å¼ + å†…åœ¨åŸå› ï¼‰\n",
        "å€™é€‰ mc ä¸æ˜¯äººå·¥å›ºå®šåˆ—è¡¨ï¼Œè€Œç”±æ–‡æ¡£è§„æ¨¡ä¸è¯­æ–™ç»Ÿè®¡è‡ªé€‚åº”ç”Ÿæˆã€‚è®¾æ–‡æ¡£æ•°ä¸º $N$ï¼š",
        "$$mc_{base} = \\alpha\\sqrt{N} + \\beta\\ln(N)$$",
        "è¿›ä¸€æ­¥ç”¨æ–‡æœ¬å¤šæ ·æ€§ï¼ˆè¯æ±‡é¦™å†œç†µå½’ä¸€åŒ–ï¼‰ä¸å‘é‡ç©ºé—´å¯†åº¦åšè°ƒæ•´ï¼š",
        "$$mc_{adj} = mc_{base}\\cdot(1+\\gamma H_{vocab}^{norm}+\\delta\\,density_{factor})$$",
        "æœ€åå–å¤šä¸ªç¼©æ”¾ç‰ˆæœ¬å¹¶è£å‰ªåˆ°åˆç†ä¸Šç•Œå¾—åˆ°å€™é€‰é›†ï¼ˆå¹¶å»é‡ï¼‰ï¼š",
        "$$mc\\in\\{1.3\\,mc_{adj},\\ 1.0\\,mc_{adj},\\ 0.7\\,mc_{adj},\\ 0.4\\,mc_{adj}\\}$$",
        "**ä¸ºä»€ä¹ˆå€™é€‰å¸¸å¸¸çœ‹èµ·æ¥â€˜é‡å¤â€™**ï¼šä¸åŒå»å™ªæ–¹æ³•è‹¥ $N$ ä¸ç»Ÿè®¡ç‰¹å¾æ¥è¿‘ï¼Œä¸Šå¼ä¼šå¾—åˆ°ç›¸è¿‘çš„ $mc_{adj}$ï¼Œç¦»æ•£åŒ–ï¼ˆå–æ•´+clipï¼‰åè‡ªç„¶è½åœ¨ç›¸åŒçš„æ•´æ•°é›†åˆä¸­ã€‚ä¸ºä¿è¯å¤ç°ï¼Œæˆ‘ä»¬å›ºå®šäº†å¯†åº¦æŠ½æ ·çš„éšæœºç§å­ã€‚\n",

        "#### ç¬¦å·è¡¨ï¼ˆNotationï¼‰\n",
        "| ç¬¦å· | å«ä¹‰ |",
        "|---|---|",
        "| $N$ | æ–‡æ¡£æ•° |",
        "| $mc$ / $mc_i$ | HDBSCAN çš„ min\\_cluster\\_sizeï¼ˆå€™é€‰è¶…å‚æ•°ï¼‰ |",
        "| $c_i$ | $C_v(mc_i)$ï¼Œä¸»é¢˜ä¸€è‡´æ€§ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ |",
        "| $n_i$ | $noise(mc_i)$ï¼Œå™ªéŸ³æ¯”ä¾‹ï¼ˆTopic=-1 å æ¯”ï¼Œè¶Šå°è¶Šå¥½ï¼‰ |",
        "| $r$ | å™ªéŸ³çº¦æŸé˜ˆå€¼ï¼ˆæ¥è‡ªçˆ¶æ–¹æ³•çš„æœ€å°å™ªéŸ³ï¼‰ |",
        "| $F(r)$ | å¯è¡Œé›† $\\{i\\mid n_i\\le r\\}$ |",
        "| $Pareto(\\cdot)$ | ä¸è¢«å…¶å®ƒå€™é€‰æ”¯é…çš„é›†åˆï¼ˆåŒç›®æ ‡ï¼šmax $c$ / min $n$ï¼‰ |\n",

        *_reproducibility_section(base_dir, method, best_mc=best_mc),

        "### å‰æ²¿åˆ†ç±»è§„åˆ™\n",
        "åŸºäº Small et al. (2014) å’Œ Chen (2006) çš„ç ”ç©¶å‰æ²¿ç†è®ºï¼š\n",
        "| åˆ†ç±» | ä¸­æ–‡ | åˆ¤å®šæ¡ä»¶ | ç†è®ºä¾æ® |",
        "|-----|-----|---------|---------|",
        "| ğŸ”¥ **Hotspot** | çƒ­ç‚¹ | Composite â‰¥ 50% ä¸” Heat â‰¥ 50% | Chen (2006) citation burst |",
        "| ğŸŒ± **Emerging** | æ–°å…´ | Novelty â‰¥ 60% ä¸” Growth â‰¥ 30% | Small et al. (2014) |",
        "| ğŸ’ **Potential** | æ½œåœ¨ | Novelty â‰¥ 75% ä¸” Composite < 50% | Shibata et al. (2008) |",
        "| ğŸ“‰ **Declining** | è¡°é€€ | Novelty < 60% ä¸” Heat < 50% | é€†å‘æ¨æ–­ |",
        "| â– **General** | ä¸€èˆ¬ | å…¶ä»–æƒ…å†µ | ç¨³å®šå¸¸è§„é¢†åŸŸ |\n",
        "",
    ]


def _key_findings_section(base_dir: Path, method: str, best_mc: int, topic_phrase_map: Optional[Dict[int, str]] = None) -> List[str]:
    """ç”Ÿæˆå…³é”®å‘ç°ç« èŠ‚"""
    lines: List[str] = []
    lines.append("## å…³é”®å‘ç°ä¸ç ”ç©¶å»ºè®®\n")
    
    files = _pick_best_files(base_dir, method, best_mc)
    frontier_file = files["frontier"]
    
    if not frontier_file.exists():
        lines.append("- æœªæ‰¾åˆ°å‰æ²¿æŒ‡æ ‡æ–‡ä»¶\n")
        return lines
    
    try:
        df = pd.read_csv(frontier_file)
    except:
        return lines
    
    # ç»Ÿè®¡å‰æ²¿ç±»å‹åˆ†å¸ƒ
    if "Frontier_Type" in df.columns:
        type_counts = df["Frontier_Type"].value_counts().to_dict()
        total = len(df)
        
        lines.append("### å‰æ²¿ä¸»é¢˜åˆ†ç±»ç»Ÿè®¡\n")
        lines.append("| ç±»å‹ | æ•°é‡ | å æ¯” | è¯´æ˜ |")
        lines.append("|-----|-----|-----|------|")
        
        type_info = {
            "çƒ­ç‚¹": ("ğŸ”¥", "å½“å‰å­¦æœ¯ç•Œé«˜åº¦å…³æ³¨çš„ç ”ç©¶æ–¹å‘"),
            "æ–°å…´": ("ğŸŒ±", "è¿‘å¹´å¿«é€Ÿå‘å±•çš„æ–°å…´é¢†åŸŸ"),
            "æ½œåœ¨": ("ğŸ’", "å…·æœ‰å‘å±•æ½œåŠ›çš„å‰æ²¿æ–¹å‘"),
            "è¡°é€€": ("ğŸ“‰", "ç ”ç©¶çƒ­åº¦ä¸‹é™çš„ä¼ ç»Ÿé¢†åŸŸ"),
            "ä¸€èˆ¬": ("â–", "å¸¸è§„ç¨³å®šçš„ç ”ç©¶é¢†åŸŸ"),
        }
        
        for ft, (emoji, desc) in type_info.items():
            count = type_counts.get(ft, 0)
            pct = count / total * 100 if total > 0 else 0
            lines.append(f"| {emoji} {ft} | {count} | {pct:.1f}% | {desc} |")
        
        lines.append("")
    
    # è¾…åŠ©å‡½æ•°ï¼šä»TopWordsç”ŸæˆçŸ­è¯­
    def _get_phrase_from_row(row):
        try:
            tid = int(row.get("Topic"))
        except Exception:
            tid = None
        if tid is not None and topic_phrase_map and tid in topic_phrase_map:
            return topic_phrase_map[tid]
        keywords = _extract_keywords_from_topwords(row.get("TopWords", ""), limit=8)
        return _generate_topic_phrase(keywords)
    
    # è¯†åˆ«Topçƒ­ç‚¹
    if "Heat_RecentRatio" in df.columns and "TopWords" in df.columns:
        lines.append("### Current Research Hotspots (Heat Top 5)\n")
        hot_topics = df.nlargest(5, "Heat_RecentRatio")
        for i, (_, row) in enumerate(hot_topics.iterrows(), 1):
            phrase = _get_phrase_from_row(row)
            heat = row["Heat_RecentRatio"]
            lines.append(f"{i}. **{phrase}** (Recent ratio: {heat:.1%})")
        lines.append("")
    
    # è¯†åˆ«æ–°å…´æ–¹å‘
    if "Novelty_AvgYear" in df.columns:
        lines.append("### Emerging Research Directions (Novelty Top 5)\n")
        novel_topics = df.nlargest(5, "Novelty_AvgYear")
        for i, (_, row) in enumerate(novel_topics.iterrows(), 1):
            phrase = _get_phrase_from_row(row)
            year = row["Novelty_AvgYear"]
            lines.append(f"{i}. **{phrase}** (Avg. year: {year:.1f})")
        lines.append("")
    
    # é«˜å½±å“åŠ›ä¸»é¢˜
    if "Avg_Citations" in df.columns:
        lines.append("### High-Impact Research (Citations Top 5)\n")
        cited_topics = df.nlargest(5, "Avg_Citations")
        for i, (_, row) in enumerate(cited_topics.iterrows(), 1):
            phrase = _get_phrase_from_row(row)
            cit = row["Avg_Citations"]
            lines.append(f"{i}. **{phrase}** (Avg. citations: {cit:.1f})")
        lines.append("")
    
    lines.append("### Research Recommendations\n")
    lines.append("1. **Focus on Hotspots**: Track Heat Top 10 topics - these are currently highly active research areas")
    lines.append("2. **Identify Emerging Trends**: High Novelty topics represent the latest research trends")
    lines.append("3. **Study High-Impact Work**: Topics with high citations contain seminal papers worth reading")
    lines.append("4. **Explore Potential Topics**: Potential-type topics may become future hotspots")
    lines.append("")
    
    return lines


# ä¸»é¢˜çŸ­è¯­æ¨¡æ¿ï¼ˆåŸºäºå…³é”®è¯ç»„åˆç”Ÿæˆä¸“ä¸šæè¿°ï¼‰
TOPIC_PHRASE_TEMPLATES = {
    # æ²»ç–—ç›¸å…³
    ("therapy", "eradication"): "H. pylori eradication therapy",
    ("eradication", "triple"): "Triple therapy for H. pylori eradication",
    ("eradication", "quadruple"): "Quadruple therapy regimens",
    ("treatment", "resistance"): "Antibiotic resistance in treatment",
    ("vonoprazan", "eradication"): "Vonoprazan-based eradication therapy",
    ("probiotics", "eradication"): "Probiotic-supplemented eradication therapy",
    ("medicine", "chinese"): "Traditional Chinese medicine therapy",
    ("decoction", "chinese"): "Chinese herbal decoction treatment",
    ("guidelines", "consensus"): "Clinical guidelines and consensus",
    ("management", "guidelines"): "Clinical management guidelines",
    
    # ç™Œç—‡ç›¸å…³
    ("cancer", "gastric"): "Gastric cancer pathogenesis",
    ("cancer", "risk"): "Cancer risk factors",
    ("cancer", "incidence"): "Cancer incidence and trends",
    ("cancer", "mortality"): "Cancer incidence and mortality trends",
    ("adenocarcinoma", "gastric"): "Gastric adenocarcinoma",
    ("carcinogenesis", "gastric"): "Gastric carcinogenesis mechanisms",
    ("tumor", "immune"): "Tumor immune microenvironment",
    ("immunotherapy", "cancer"): "Cancer immunotherapy",
    ("immunotherapy", "pdl1"): "PD-L1/PD-1 immunotherapy",
    ("lncrnas", "cancer"): "LncRNA in cancer progression",
    ("lncrna", "expression"): "LncRNA expression profiling",
    
    # æºƒç–¡ç›¸å…³
    ("ulcer", "peptic"): "Peptic ulcer disease",
    ("ulcer", "bleeding"): "Peptic ulcer bleeding",
    ("ulcer", "duodenal"): "Duodenal ulcer",
    ("nsaid", "ulcer"): "NSAID-induced ulcer",
    ("aspirin", "bleeding"): "Aspirin-associated GI bleeding",
    
    # æ·‹å·´ç˜¤ç›¸å…³
    ("lymphoma", "malt"): "MALT lymphoma",
    ("lymphoma", "gastric"): "Gastric lymphoma",
    
    # æ¯’åŠ›å› å­
    ("caga", "vaca"): "CagA/VacA virulence factors",
    ("caga", "positive"): "CagA-positive strains",
    ("virulence", "factors"): "Virulence factor analysis",
    ("genotypes", "virulence"): "Virulence genotyping",
    
    # å¾®ç”Ÿç‰©ç»„
    ("microbiota", "gut"): "Gut microbiota interaction",
    ("microbiome", "gastric"): "Gastric microbiome",
    ("dysbiosis", "microbiota"): "Microbiota dysbiosis",
    
    # è¯Šæ–­ä¸æ£€æµ‹
    ("diagnosis", "endoscopy"): "Endoscopic diagnosis",
    ("detection", "molecular"): "Molecular detection methods",
    ("detection", "electrochemical"): "Electrochemical biosensor detection",
    ("detection", "lamp"): "LAMP-based rapid detection",
    ("detection", "dna"): "DNA-based detection methods",
    ("test", "urea"): "Urea breath test",
    ("biopsy", "histology"): "Histological biopsy analysis",
    ("ai", "detection"): "AI-assisted diagnosis",
    ("ai", "images"): "AI-based image analysis",
    ("images", "learning"): "Machine learning image analysis",
    ("artificial", "intelligence"): "Artificial intelligence applications",
    
    # ç‚ç—‡ä¸å…ç–«
    ("gastritis", "chronic"): "Chronic gastritis",
    ("gastritis", "atrophic"): "Atrophic gastritis",
    ("inflammation", "gastric"): "Gastric inflammation",
    ("immune", "response"): "Immune response mechanisms",
    ("cytokines", "inflammation"): "Cytokine-mediated inflammation",
    ("il", "expression"): "Interleukin expression",
    ("il", "cells"): "Interleukin and immune cells",
    
    # ç»†èƒæœºåˆ¶
    ("cells", "epithelial"): "Epithelial cell responses",
    ("cells", "expression"): "Cell gene expression",
    ("apoptosis", "cells"): "Cell apoptosis mechanisms",
    ("signaling", "pathway"): "Signaling pathway analysis",
    ("expression", "genes"): "Gene expression analysis",
    
    # æµè¡Œç—…å­¦
    ("prevalence", "infection"): "Infection prevalence",
    ("prevalence", "children"): "Prevalence in children",
    ("epidemiology", "global"): "Global epidemiology",
    ("transmission", "infection"): "Transmission patterns",
    
    # è€è¯æ€§
    ("resistance", "antibiotic"): "Antibiotic resistance",
    ("resistance", "clarithromycin"): "Clarithromycin resistance",
    ("resistance", "metronidazole"): "Metronidazole resistance",
    ("resistance", "mutations"): "Antibiotic resistance mutations",
    ("mutations", "resistance"): "Resistance mutations",
    
    # å…¶ä»–ç–¾ç—…å…³è”
    ("diabetes", "mellitus"): "Diabetes mellitus association",
    ("cardiovascular", "disease"): "Cardiovascular disease link",
    ("liver", "nafld"): "NAFLD and liver disease",
    ("nafld", "fatty"): "Non-alcoholic fatty liver disease",
    ("iron", "deficiency"): "Iron deficiency anemia",
    ("pancreatic", "cancer"): "Pancreatic cancer association",
    ("pancreatitis", "pancreatic"): "Pancreatitis studies",
    
    # å¤©ç„¶äº§ç‰©ä¸æŠ—èŒ
    ("extract", "activity"): "Natural extract activity",
    ("compounds", "antibacterial"): "Antibacterial compounds",
    ("probiotics", "effects"): "Probiotic effects",
    ("probiotics", "diarrhea"): "Probiotics for diarrhea prevention",
    
    # å†…é•œç›¸å…³
    ("endoscopic", "resection"): "Endoscopic resection",
    ("esd", "submucosal"): "Endoscopic submucosal dissection",
    ("metaplasia", "intestinal"): "Intestinal metaplasia",
    ("lesion", "endoscopic"): "Endoscopic lesion management",
    
    # ç—…æ¯’ä¸æ„ŸæŸ“
    ("virus", "cancers"): "Infection-attributable cancers",
    ("hepatitis", "virus"): "Hepatitis virus co-infection",
    ("hpv", "cancer"): "HPV-related cancers",
    
    # é…¶ä¸è›‹ç™½
    ("urease", "activity"): "Urease enzyme activity",
    ("urease", "inhibitors"): "Urease inhibitors",
    ("proteins", "protein"): "Protein structure analysis",
    ("adhesion", "binding"): "Bacterial adhesion mechanisms",
    
    # å¤–è†œå›Šæ³¡ä¸åˆ†æ³Œ
    ("omvs", "vesicles"): "Outer membrane vesicles",
    ("secretion", "system"): "Type IV secretion system",
    
    # åŠ¨ç‰©æ¨¡å‹
    ("mice", "model"): "Mouse model studies",
    ("mouse", "infection"): "Mouse infection model",
    ("animal", "model"): "Animal model studies",
    
    # è¯ç‰©ä¸ä¸´åºŠ
    ("drug", "delivery"): "Drug delivery systems",
    ("drug", "efficacy"): "Drug efficacy studies",
    ("ppi", "inhibitor"): "Proton pump inhibitor therapy",
    ("amoxicillin", "clarithromycin"): "Amoxicillin-clarithromycin regimen",
    
    # åŸºå› ä¸é—ä¼ 
    ("gene", "expression"): "Gene expression profiling",
    ("polymorphisms", "risk"): "Genetic polymorphism risk",
    ("snp", "association"): "SNP association studies",
    
    # è¡€æ¸…å­¦ä¸è¯Šæ–­
    ("serology", "antibody"): "Serological antibody testing",
    ("igg", "antibody"): "IgG antibody detection",
    ("stool", "antigen"): "Stool antigen test",
    
    # åœ°åŒºç ”ç©¶
    ("chinese", "population"): "Chinese population study",
    ("asian", "population"): "Asian population study",
    ("pediatric", "children"): "Pediatric infection",
}


def _generate_topic_phrase(keywords: List[str]) -> str:
    """æ ¹æ®å…³é”®è¯ç”Ÿæˆä¸“ä¸šçš„è‹±æ–‡ä¸»é¢˜çŸ­è¯­æè¿°"""
    if not keywords:
        return "General topic"
    
    # æ ‡å‡†åŒ–å…³é”®è¯
    kw_lower = [k.lower().strip() for k in keywords[:10]]
    
    # å°è¯•åŒ¹é…é¢„å®šä¹‰æ¨¡æ¿ï¼ˆæŒ‰ä¸¤ä¸ªå…³é”®è¯ç»„åˆæŸ¥æ‰¾ï¼‰
    for (k1, k2), phrase in TOPIC_PHRASE_TEMPLATES.items():
        if k1 in kw_lower and k2 in kw_lower:
            return phrase
    
    # å•å…³é”®è¯ä¸“ä¸šæœ¯è¯­æ˜ å°„
    single_term_map = {
        "urease": "Urease enzyme studies",
        "omvs": "Outer membrane vesicles (OMVs)",
        "biofilm": "Biofilm formation",
        "adhesin": "Adhesin-mediated colonization",
        "flagella": "Flagellar motility",
        "chemotaxis": "Chemotaxis mechanisms",
        "autophagy": "Autophagy pathway",
        "apoptosis": "Apoptosis regulation",
        "vaccine": "Vaccine development",
        "nanoparticles": "Nanoparticle-based therapy",
        "curcumin": "Curcumin anti-H. pylori activity",
        "garlic": "Garlic extract antimicrobial effects",
        "honey": "Honey antibacterial properties",
        "propolis": "Propolis antimicrobial activity",
        "lactoferrin": "Lactoferrin antimicrobial effects",
    }
    
    # æ£€æŸ¥å•å…³é”®è¯åŒ¹é…
    for kw in kw_lower[:3]:
        if kw in single_term_map:
            return single_term_map[kw]
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œå°è¯•æ™ºèƒ½ç»„åˆ
    # ç­–ç•¥ï¼šæ ¸å¿ƒåè¯ + ä¿®é¥°è¯ + ç ”ç©¶ç±»å‹
    core_nouns = ["therapy", "treatment", "cancer", "ulcer", "gastritis", "lymphoma", 
                  "infection", "resistance", "microbiota", "diagnosis", "eradication",
                  "adenocarcinoma", "carcinoma", "metaplasia", "dysplasia", "inflammation",
                  "colonization", "pathogenesis", "virulence"]
    
    modifiers = ["gastric", "peptic", "chronic", "atrophic", "intestinal", "duodenal",
                 "antibiotic", "triple", "quadruple", "molecular", "endoscopic",
                 "bacterial", "mucosal", "epithelial", "systemic"]
    
    study_types = ["analysis", "mechanisms", "factors", "patterns", "effects", 
                   "response", "expression", "pathogenesis", "association", "studies",
                   "activity", "regulation", "interaction"]
    
    found_noun = None
    found_modifier = None
    found_study = None
    
    for kw in kw_lower:
        if not found_noun:
            for noun in core_nouns:
                if noun in kw:
                    found_noun = kw
                    break
        if not found_modifier:
            for mod in modifiers:
                if mod in kw:
                    found_modifier = kw
                    break
        if not found_study:
            for st in study_types:
                if st in kw:
                    found_study = kw
                    break
    
    # ç»„åˆçŸ­è¯­
    parts = []
    if found_modifier:
        parts.append(found_modifier.capitalize())
    if found_noun:
        parts.append(found_noun)
    elif keywords:
        parts.append(keywords[0])
    if found_study and len(parts) < 3:
        parts.append(found_study)
    
    if len(parts) >= 2:
        # å½¢æˆçŸ­è¯­ï¼šå¦‚ "Gastric cancer analysis"
        phrase = " ".join(parts[:3])
        return phrase.capitalize() if phrase else keywords[0].capitalize()
    
    # å¤‡é€‰ï¼šä½¿ç”¨å‰2-3ä¸ªå…³é”®è¯ç»„åˆæˆçŸ­è¯­ï¼Œæ·»åŠ  "studies" åç¼€ä½¿å…¶æ›´ä¸“ä¸š
    if len(keywords) >= 2:
        base = f"{keywords[0].capitalize()} {keywords[1]}"
        if len(keywords) >= 3 and len(keywords[2]) > 3:
            base = f"{base} {keywords[2]}"
        return base + " studies" if len(base) < 30 else base
    
    # æœ€åå¤‡é€‰ï¼šå•ä¸ªå…³é”®è¯ + "research"
    return f"{keywords[0].capitalize()} research"


def _extract_keywords_from_topwords(words_str: str, limit: int = 8) -> List[str]:
    s = str(words_str or "").replace(";", ", ").strip()
    kws = [w.strip() for w in s.split(",") if w.strip()]
    return kws[:limit]


def _extract_keywords_from_representation(rep: str, limit: int = 8) -> List[str]:
    s = str(rep or "").strip().strip("[]").replace("'", "").replace('"', "")
    kws = [w.strip() for w in s.split(",") if w.strip()]
    return kws[:limit]


def _contains_cjk(s: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", str(s or "")))


def _translate_en_phrase_to_cn(en_phrase: str) -> str:
    """å°†è‹±æ–‡çŸ­è¯­ç²—ç¿»ä¸ºä¸­æ–‡çŸ­è¯­ï¼ˆç”¨äº Topic Description çš„ä¸­æ–‡éƒ¨åˆ†ï¼‰ã€‚

    ç›®æ ‡ï¼šå¯è¯»ã€é¢†åŸŸç›¸å…³ã€å®å¯ä¿ç•™å°‘é‡è‹±æ–‡ä¹Ÿä¸èƒ¡è¯‘ã€‚
    """
    CN = {
        # general
        "study": "ç ”ç©¶",
        "studies": "ç ”ç©¶",
        "analysis": "åˆ†æ",
        "mechanism": "æœºåˆ¶",
        "mechanisms": "æœºåˆ¶",
        "prevention": "é¢„é˜²",
        "rate": "ç‡",
        "rates": "ç‡",
        "factor": "å› ç´ ",
        "factors": "å› ç´ ",
        "gut": "è‚ é“",
        "effects": "ä½œç”¨",
        "effect": "ä½œç”¨",
        "association": "å…³è”",
        "associations": "å…³è”",
        "risk": "é£é™©",
        "management": "ç®¡ç†",
        "guidelines": "æŒ‡å—",
        "consensus": "å…±è¯†",
        "response": "ååº”",
        "expression": "è¡¨è¾¾",
        "interaction": "ç›¸äº’ä½œç”¨",
        "regulation": "è°ƒæ§",
        "triple": "ä¸‰è”",
        "quadruple": "å››è”",
        "patterns": "æ¨¡å¼",
        "pathogenesis": "å‘ç—…æœºåˆ¶",

        # domain
        "microbial": "å¾®ç”Ÿç‰©",
        "gastric": "èƒƒ",
        "cancer": "ç™Œ",
        "carcinoma": "ç™Œ",
        "adenocarcinoma": "è…ºç™Œ",
        "ulcer": "æºƒç–¡",
        "peptic": "æ¶ˆåŒ–æ€§",
        "gastritis": "èƒƒç‚",
        "lymphoma": "æ·‹å·´ç˜¤",
        "malt": "MALT",
        "therapy": "æ²»ç–—",
        "treatment": "æ²»ç–—",
        "eradication": "æ ¹é™¤",
        "antibiotic": "æŠ—ç”Ÿç´ ",
        "resistance": "è€è¯",
        "microbiota": "å¾®ç”Ÿç‰©ç¾¤",
        "microbiome": "å¾®ç”Ÿç‰©ç»„",
        "diagnosis": "è¯Šæ–­",
        "detection": "æ£€æµ‹",

        # epidemiology / natural products
        "infection": "æ„ŸæŸ“",
        "prevalence": "æ‚£ç—…ç‡",
        "natural": "å¤©ç„¶",
        "extract": "æå–ç‰©",
        "extracts": "æå–ç‰©",
        "activity": "æ´»æ€§",
        "endoscopic": "å†…é•œ",
        "endoscopy": "å†…é•œ",
        "immune": "å…ç–«",
        "immunotherapy": "å…ç–«æ²»ç–—",
        "tumor": "è‚¿ç˜¤",
        "biofilm": "ç”Ÿç‰©è†œ",
        "probiotic": "ç›Šç”ŸèŒ",
        "probiotics": "ç›Šç”ŸèŒ",
        "urease": "å°¿ç´ é…¶",
        "virulence": "æ¯’åŠ›",
        "vaccine": "ç–«è‹—",
        "nanoparticle": "çº³ç±³é¢—ç²’",
        "nanoparticles": "çº³ç±³é¢—ç²’",
        "drug": "è¯ç‰©",
        "delivery": "é€’é€",
        "ppi": "PPI",

        # organism
        "helicobacter": "å¹½é—¨èºæ†èŒ",
        "pylori": "å¹½é—¨èºæ†èŒ",
        "h": "å¹½é—¨èºæ†èŒ",
    }

    phrase = str(en_phrase or "").strip()
    # é¢„è§„èŒƒåŒ–ï¼šå°† H. pylori è§†ä¸ºä¸€ä¸ªæ•´ä½“æ¦‚å¿µï¼Œé¿å… h + pylori é‡å¤ç¿»è¯‘
    phrase = re.sub(r"\bH\.?\s*pylori\b", "HPYLORI", phrase, flags=re.IGNORECASE)
    # å¤„ç†åˆ†éš”ç¬¦
    phrase = phrase.replace("â€”", "-")
    parts = [p.strip() for p in phrase.split("-") if p.strip()]

    def _render_part(part: str) -> str:
        tokens = re.findall(r"[A-Za-z0-9]+", part)
        mapped = []
        for tok in tokens:
            low = tok.lower()
            if low == "hpylori":
                mapped.append("å¹½é—¨èºæ†èŒ")
                continue
            if low in ("caga", "vaca"):
                mapped.append(tok)
                continue
            mapped.append(CN.get(low, tok))

        # è¿å†™ä¸­æ–‡ï¼Œä¿ç•™å¿…è¦ç©ºæ ¼
        out = ""
        for t in mapped:
            if not out:
                out = t
                continue
            if _contains_cjk(out[-1]) and _contains_cjk(t[:1]):
                out += t
            else:
                out += " " + t
        # ç‰¹ä¾‹ï¼š"èƒƒ"+"ç™Œ" -> "èƒƒç™Œ"
        out = out.replace("èƒƒ ç™Œ", "èƒƒç™Œ")
        # ç¾åŒ–ï¼šä¸­æ–‡åˆ†éš”ç¬¦
        out = out.replace(",", "ã€")
        out = re.sub(r"\s+", " ", out).strip()
        return out

    cn_parts = [_render_part(p) for p in parts]
    cn = "â€”".join([p for p in cn_parts if p])
    cn = cn.replace("å¹½é—¨èºæ†èŒ å¹½é—¨èºæ†èŒ", "å¹½é—¨èºæ†èŒ")
    cn = cn.replace("å¹½é—¨èºæ†èŒå¹½é—¨èºæ†èŒ", "å¹½é—¨èºæ†èŒ")
    cn = re.sub(r"(å¹½é—¨èºæ†èŒ){2,}", "å¹½é—¨èºæ†èŒ", cn)
    return cn.strip()


def _make_bilingual_topic_desc(en_phrase: str, fallback_keywords: List[str]) -> str:
    cn = _translate_en_phrase_to_cn(en_phrase)
    if not _contains_cjk(cn):
        # å›é€€ï¼šä»å…³é”®è¯é‡Œå°½é‡æ‹¼å‡ºä¸­æ–‡ï¼ˆä¿ç•™å°‘é‡è‹±æ–‡ä¹Ÿå¯ä»¥ï¼‰
        kws = [k.strip() for k in (fallback_keywords or []) if k.strip()]
        cn_kws = _translate_en_phrase_to_cn(" ".join(kws[:4]))
        cn = cn_kws if _contains_cjk(cn_kws) else "ç ”ç©¶ä¸»é¢˜"
    # ä½¿ç”¨ä¸­æ–‡å…¨è§’æ‹¬å·ï¼Œé¿å…è¢« step11 è¯¯åˆ¤æˆå†æ¬¡æ›¿æ¢
    return f"{cn}ï¼ˆ{en_phrase}ï¼‰"


def _build_unique_topic_phrase_map(topic_info_df: pd.DataFrame) -> Dict[int, str]:
    """ä¸ºåŒä¸€æ–¹æ³•å†…çš„æ‰€æœ‰ Topic ç”Ÿæˆç¨³å®šä¸”å”¯ä¸€çš„ä¸»é¢˜çŸ­è¯­ã€‚

    ä¸¥è°¨åŸåˆ™ï¼š
    - å…ˆæŒ‰æ—¢æœ‰è§„åˆ™ç”ŸæˆçŸ­è¯­ï¼ˆä¿æŒå¯è¯»æ€§ä¸ä¸€è‡´æ€§ï¼‰
    - è‹¥å‡ºç°é‡å¤çŸ­è¯­ï¼ˆä¸åŒ Topic ç”ŸæˆåŒä¸€ phraseï¼‰ï¼Œåˆ™ä¼˜å…ˆè¿½åŠ â€œå·®å¼‚å…³é”®è¯â€æ¶ˆæ­§ï¼›ä»å†²çªåˆ™è¿½åŠ  Variant åºå·ï¼ˆä¸æš´éœ² Topic ç¼–å·ï¼‰
    - åŒä¸€ Topic åœ¨æ•´ä»½æŠ¥å‘Šçš„ä»»ä½•ä½ç½®éƒ½ä½¿ç”¨åŒä¸€ä¸ªçŸ­è¯­
    """
    phrase_by_topic: Dict[int, str] = {}
    topics_by_phrase: Dict[str, List[int]] = {}

    if topic_info_df is None or topic_info_df.empty or "Topic" not in topic_info_df.columns:
        return phrase_by_topic

    keywords_by_topic: Dict[int, List[str]] = {}
    keyword_freq: Dict[str, int] = {}

    for _, row in topic_info_df.iterrows():
        try:
            topic_id = int(row.get("Topic"))
        except Exception:
            continue
        if topic_id < 0:
            continue

        if "Representation" in topic_info_df.columns:
            keywords = _extract_keywords_from_representation(row.get("Representation", ""), limit=10)
        elif "TopWords" in topic_info_df.columns:
            keywords = _extract_keywords_from_topwords(row.get("TopWords", ""), limit=10)
        else:
            keywords = []

        keywords_by_topic[topic_id] = keywords
        for w in keywords:
            wl = str(w or "").strip().lower()
            if wl:
                keyword_freq[wl] = keyword_freq.get(wl, 0) + 1

        base_phrase_en = _generate_topic_phrase(keywords)
        phrase_by_topic[topic_id] = base_phrase_en
        topics_by_phrase.setdefault(base_phrase_en, []).append(topic_id)

    # ä¸¥æ ¼æ¶ˆæ­§ï¼ˆè¯­ä¹‰åŒ–ä¼˜å…ˆï¼‰ï¼šåŒ phrase å¤š topic æ—¶ï¼Œç”¨â€œä½é¢‘å·®å¼‚å…³é”®è¯â€ï¼ˆè¿‘ä¼¼ IDFï¼‰åšåŒºåˆ†ï¼Œä»å†²çªåˆ™è¿½åŠ  Variant åºå·
    for base_phrase, topic_ids in topics_by_phrase.items():
        if len(topic_ids) <= 1:
            continue

        # base_phrase çš„è¯é›†åˆï¼ˆç”¨äºæ’é™¤â€œå·²ç»è¡¨è¾¾è¿‡â€çš„è¯ï¼‰
        base_tokens = set([t.lower() for t in re.findall(r"[A-Za-z0-9]+", base_phrase)])

        new_phrases: Dict[int, str] = {}
        for tid in topic_ids:
            words = [w.strip() for w in (keywords_by_topic.get(tid) or []) if str(w).strip()]
            # é€‰æ‹©ä½é¢‘è¯ä¼˜å…ˆï¼ˆæ›´èƒ½åŒºåˆ†ä¸»é¢˜ï¼‰ï¼Œå¹¶æ’é™¤ base_phrase å·²å«è¯
            scored = []
            for w in words:
                wl = w.lower()
                if wl in base_tokens:
                    continue
                scored.append((keyword_freq.get(wl, 999), words.index(w), w))
            scored.sort(key=lambda x: (x[0], x[1]))
            extra = [x[2] for x in scored[:2] if x[2]]

            if extra:
                new_phrases[tid] = f"{base_phrase} â€” {', '.join(extra)}"
            else:
                new_phrases[tid] = base_phrase

        # æ£€æŸ¥æ˜¯å¦å·²æ¶ˆæ­§
        inv: Dict[str, List[int]] = {}
        for tid, phr in new_phrases.items():
            inv.setdefault(phr, []).append(tid)
        still_dup = {phr: tids for phr, tids in inv.items() if len(tids) > 1}
        if not still_dup:
            for tid, phr in new_phrases.items():
                phrase_by_topic[tid] = phr
            continue

        # å…œåº•ï¼šä»é‡å¤åˆ™è¿½åŠ  Variant åºå·ï¼ˆä¸æ˜¾ç¤º Topic IDï¼‰
        for i, tid in enumerate(sorted(topic_ids)):
            phrase_by_topic[tid] = f"{new_phrases.get(tid, base_phrase)} â€” v{i+1}"

    # æœ€ç»ˆè¾“å‡ºï¼šä¸­æ–‡ï¼ˆè‹±æ–‡ï¼‰åŒè¯­ï¼ˆç›´æ¥å†™å…¥æŠ¥å‘Šï¼Œé¿å…åç»­ç¿»è¯‘ä¸å…¨/è‹±æ–‡(è‹±æ–‡)ï¼‰
    bilingual: Dict[int, str] = {}
    for tid, en in phrase_by_topic.items():
        bilingual[tid] = _make_bilingual_topic_desc(en, fallback_keywords=keywords_by_topic.get(tid, []))

    return bilingual


def _method_frontier_metrics_section(base_dir: Path, method: str, best_mc: int, topic_phrase_map: Optional[Dict[int, str]] = None) -> List[str]:
    """ç”Ÿæˆè¯¥æ–¹æ³•çš„ç ”ç©¶å‰æ²¿äº”ä¸ªæŒ‡æ ‡ Top 10 æ’å"""
    lines: List[str] = []
    lines.append("## ç ”ç©¶å‰æ²¿æŒ‡æ ‡åˆ†æ\n")
    lines.append("æŒ‰äº”ä¸ªå…³é”®æŒ‡æ ‡æ’åºï¼Œæ¯ä¸ªæŒ‡æ ‡å±•ç¤ºå¾—åˆ†æœ€é«˜çš„ 10 ä¸ªä¸»é¢˜ã€‚\n")
    lines.append("**æŒ‡æ ‡è¯´æ˜**ï¼š")
    lines.append("- **Strengthï¼ˆå¼ºåº¦ï¼‰**: ä¸»é¢˜è§„æ¨¡å æ¯”ï¼Œåæ˜ ç ”ç©¶é¢†åŸŸçš„é‡è¦ç¨‹åº¦")
    lines.append("- **Noveltyï¼ˆæ–°é¢–æ€§ï¼‰**: å¹³å‡å‘è¡¨å¹´ä»½ï¼Œå¹´ä»½è¶Šæ–°è¡¨ç¤ºç ”ç©¶è¶Šå‰æ²¿")
    lines.append("- **Heatï¼ˆçƒ­ç‚¹ï¼‰**: è¿‘3å¹´æ–‡çŒ®å æ¯”ï¼Œåæ˜ å½“å‰å­¦æœ¯å…³æ³¨åº¦")
    lines.append("- **Avg_Citationsï¼ˆå¼•ç”¨åº¦ï¼‰**: å¹³å‡è¢«å¼•ç”¨æ¬¡æ•°ï¼Œåæ˜ å­¦æœ¯å½±å“åŠ›")
    lines.append("- **HighCited_Countï¼ˆé«˜è¢«å¼•ï¼‰**: é«˜è¢«å¼•æ–‡çŒ®æ•°ï¼ˆâ‰¥30æ¬¡ï¼‰ï¼Œåæ˜ ç»å…¸ç¨‹åº¦\n")
    
    files = _pick_best_files(base_dir, method, best_mc)
    frontier_file = files["frontier"]
    
    if not frontier_file.exists():
        lines.append("- æœªæ‰¾åˆ°ç ”ç©¶å‰æ²¿æŒ‡æ ‡æ–‡ä»¶ï¼ˆè¯·å…ˆè¿è¡Œ step07_topic_model.pyï¼‰\n")
        return lines
    
    try:
        df = pd.read_csv(frontier_file)
    except Exception as e:
        lines.append(f"- æ— æ³•è¯»å–å‰æ²¿æŒ‡æ ‡æ–‡ä»¶ï¼š{str(e)}\n")
        return lines
    
    # äº”ä¸ªæŒ‡æ ‡çš„ä¸­è‹±æ–‡æ ‡ç­¾
    metrics = {
        "Strength": {"cn": "å¼ºåº¦ï¼ˆæ–‡çŒ®æ•°é‡å æ¯”ï¼‰", "format": ".4f"},
        "Novelty_AvgYear": {"cn": "æ–°é¢–æ€§ï¼ˆå¹³å‡å‘è¡¨å¹´ä»½ï¼‰", "format": ".1f"},
        "Heat_RecentRatio": {"cn": "çƒ­ç‚¹ï¼ˆè¿‘æœŸæ–‡çŒ®å æ¯”ï¼‰", "format": ".2%"},
        "Avg_Citations": {"cn": "å¼•ç”¨åº¦ï¼ˆå¹³å‡è¢«å¼•ç”¨æ¬¡æ•°ï¼‰", "format": ".2f"},
        "HighCited_Count": {"cn": "é«˜è¢«å¼•ï¼ˆé«˜è¢«å¼•æ–‡çŒ®æ•°ï¼‰", "format": ".0f"},
    }
    
    for metric_col, metric_info in metrics.items():
        lines.append(f"\n### {metric_col} - {metric_info['cn']}\n")
        
        # æ’åºå¹¶å– Top 10
        if metric_col not in df.columns:
            lines.append(f"- åˆ— {metric_col} ä¸å­˜åœ¨\n")
            continue
        
        df_sorted = df[["Topic", "TopWords", metric_col]].dropna().sort_values(metric_col, ascending=False).head(10)
        
        lines.append("| Rank | Topicï¼ˆlabelï¼‰ | Score | Keywords |")
        lines.append("|------|---------------|-------|----------|")
        
        for rank, (_, row) in enumerate(df_sorted.iterrows(), 1):
            topic_id = int(row["Topic"])
            score_val = row[metric_col]
            
            # æ ¼å¼åŒ–å¾—åˆ†
            fmt = metric_info['format']
            if fmt == ".4f":
                score_str = f"{score_val:.4f}"
            elif fmt == ".1f":
                score_str = f"{score_val:.1f}"
            elif fmt == ".2%":
                score_str = f"{score_val:.2%}"
            elif fmt == ".2f":
                score_str = f"{score_val:.2f}"
            else:
                score_str = f"{score_val:.0f}"
            
            # è·å–å…³é”®è¯å¹¶ç”Ÿæˆä¸“ä¸šçŸ­è¯­æè¿°ï¼ˆä¼˜å…ˆä½¿ç”¨å…¨å±€å”¯ä¸€æ˜ å°„ï¼‰
            if topic_phrase_map and topic_id in topic_phrase_map:
                topic_phrase = topic_phrase_map[topic_id]
            else:
                keywords = _extract_keywords_from_topwords(row.get("TopWords", ""), limit=12)
                topic_phrase = _generate_topic_phrase(keywords)
            
            kw = ", ".join(_extract_keywords_from_topwords(row.get("TopWords", ""), limit=8))
            lines.append(f"| {rank} | {topic_phrase} | {score_str} | {kw} |")
        
        lines.append("")
    
    return lines


def _method_top_topics_section(base_dir: Path, method: str, best_mc: int, topic_phrase_map: Optional[Dict[int, str]] = None) -> List[str]:
    """ç”Ÿæˆè¯¥æ–¹æ³•çš„ä¸»è¦ä¸»é¢˜æ¦‚è§ˆ"""
    lines: List[str] = []
    lines.append("## ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼ˆè¯¥æ–¹æ³•ç‰¹æœ‰å‘ç°ï¼‰\n")
    
    files = _pick_best_files(base_dir, method, best_mc)
    ti = files["topic_info"]
    
    if not ti.exists():
        lines.append("- ä¸»é¢˜æ•°æ®æš‚æ— \n")
        return lines
    
    try:
        dft = pd.read_csv(ti)
        
        # è¿‡æ»¤æœ‰æ•ˆä¸»é¢˜ï¼ˆTopic >= 0ï¼‰
        valid_topics = dft[dft["Topic"] >= 0].copy()
        
        if valid_topics.empty:
            lines.append("- æœªæ‰¾åˆ°æœ‰æ•ˆä¸»é¢˜\n")
            return lines
        
        # æŒ‰ Count æ’åºï¼Œå– Top 10
        top_topics = valid_topics.sort_values("Count", ascending=False).head(10)
        
        lines.append(f"- Total **{len(valid_topics)}** valid topics identified")
        lines.append(f"- Top 10 topics by document count:\n")
        
        for idx, (_, row) in enumerate(top_topics.iterrows(), 1):
            topic_id = int(row["Topic"])
            count = int(row["Count"])
            
            # æå–å…³é”®è¯
            keywords = _extract_keywords_from_representation(row.get("Representation", ""), limit=8)

            # ç”Ÿæˆä¸“ä¸šçŸ­è¯­æè¿°ï¼ˆä¼˜å…ˆä½¿ç”¨å…¨å±€å”¯ä¸€æ˜ å°„ï¼‰
            if topic_phrase_map and topic_id in topic_phrase_map:
                topic_phrase = topic_phrase_map[topic_id]
            else:
                topic_phrase = _generate_topic_phrase(keywords)
            
            # å…³é”®è¯æ˜¾ç¤ºï¼ˆè‹±æ–‡ï¼Œå‰6ä¸ªï¼‰
            en_keywords = ", ".join(keywords[:6])
            
            lines.append(f"{idx}. **{topic_phrase}**")
            lines.append(f"   - Documents: {count}")
            lines.append(f"   - Keywords: {en_keywords}\n")
        
        lines.append("")
    
    except Exception as e:
        lines.append(f"- è§£æä¸»é¢˜æ—¶å‡ºé”™: {str(e)[:100]}\n")
    
    return lines


def _convert_md_to_html(md_content: str, title: str) -> str:
    """å°† Markdown è½¬æ¢ä¸ºæ¼‚äº®çš„ HTML é¡µé¢"""
    # åŸºç¡€ HTML è½¬æ¢ï¼ˆä¸ä¾èµ– markdown åº“ï¼‰
    html_body = md_content
    
    # æ ‡é¢˜è½¬æ¢
    html_body = re.sub(r'^# (.+)$', r'<h1>\1</h1>', html_body, flags=re.MULTILINE)
    html_body = re.sub(r'^## (.+)$', r'<h2>\1</h2>', html_body, flags=re.MULTILINE)
    html_body = re.sub(r'^### (.+)$', r'<h3>\1</h3>', html_body, flags=re.MULTILINE)
    
    # ä»£ç å—
    html_body = re.sub(r'```(\w+)?\n(.*?)```', r'<pre><code>\2</code></pre>', html_body, flags=re.DOTALL)
    
    # åˆ—è¡¨é¡¹
    html_body = re.sub(r'^- (.+)$', r'<li>\1</li>', html_body, flags=re.MULTILINE)
    
    # å›¾ç‰‡
    html_body = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', r'<img src="\2" alt="\1" style="max-width:100%;">', html_body)
    
    # ç²—ä½“
    html_body = re.sub(r'\*\*(.+?)\*\*', r'<strong>\1</strong>', html_body)
    
    # æ¢è¡Œ
    html_body = html_body.replace('\n\n', '</p><p>')
    
    # å¦‚æœæœ‰ markdown åº“ï¼Œä½¿ç”¨å®ƒåšæ›´å¥½çš„è½¬æ¢
    if HAS_MARKDOWN:
        try:
            html_body = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])
        except:
            pass  # ä½¿ç”¨åŸºç¡€è½¬æ¢
    
    html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <!-- KaTeX for math rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, {{delimiters: [{{left: '$$', right: '$$', display: true}}, {{left: '$', right: '$', display: false}}]}});"></script>
    <style>
        :root {{
            --primary-color: #2563eb;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --border-color: #e2e8f0;
        }}
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background: var(--bg-color);
            padding: 2rem;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: var(--card-bg);
            padding: 3rem;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.1), 0 2px 4px -1px rgba(0,0,0,0.06);
        }}
        h1 {{
            color: var(--primary-color);
            font-size: 2rem;
            margin-bottom: 1.5rem;
            padding-bottom: 1rem;
            border-bottom: 3px solid var(--primary-color);
        }}
        h2 {{
            color: var(--text-color);
            font-size: 1.4rem;
            margin: 2rem 0 1rem 0;
            padding: 0.5rem 0;
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
            background: linear-gradient(90deg, #eff6ff 0%, transparent 100%);
        }}
        h3 {{
            font-size: 1.1rem;
            margin: 1.5rem 0 0.75rem 0;
            color: #475569;
        }}
        p {{
            margin: 0.75rem 0;
        }}
        ul, ol {{
            margin: 1rem 0;
            padding-left: 1.5rem;
        }}
        li {{
            margin: 0.5rem 0;
        }}
        pre {{
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-size: 0.9rem;
        }}
        code {{
            font-family: "Fira Code", "Monaco", "Consolas", monospace;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }}
        th, td {{
            border: 1px solid var(--border-color);
            padding: 0.75rem 1rem;
            text-align: left;
        }}
        th {{
            background: #f1f5f9;
            font-weight: 600;
        }}
        tr:nth-child(even) {{
            background: #f8fafc;
        }}
        tr:hover {{
            background: #eff6ff;
        }}
        img {{
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        strong {{
            color: var(--primary-color);
        }}
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }}
        .stat-card {{
            background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);
            padding: 1.25rem;
            border-radius: 8px;
            text-align: center;
        }}
        .stat-value {{
            font-size: 1.75rem;
            font-weight: 700;
            color: var(--primary-color);
        }}
        .stat-label {{
            font-size: 0.85rem;
            color: #64748b;
            margin-top: 0.25rem;
        }}
        .footer {{
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid var(--border-color);
            text-align: center;
            color: #94a3b8;
            font-size: 0.85rem;
        }}
        @media print {{
            body {{
                background: white;
                padding: 0;
            }}
            .container {{
                box-shadow: none;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        {html_body}
        <div class="footer">
            Generated by Topic Modeling Pipeline | {time.strftime('%Y-%m-%d %H:%M:%S')}
        </div>
    </div>
</body>
</html>'''
    return html


def build_report_for_method(
    base_dir: Path,
    method: str,
    *,
    step08_ctx: Optional[Dict[str, Any]] = None,
    refresh_viz: bool = True,
    refresh_reference_mtime: float = 0.0,
) -> Dict[str, Any]:
    # load manifests
    sw_manifest = _safe_load_json(base_dir / "05_stopwords" / "stopwords_manifest.json")
    denoise_manifest = _safe_load_json(base_dir / "06_denoised_data" / "denoise_manifest.json")
    best_map = _safe_load_json(base_dir / "08_model_selection" / "best_mc_by_method.json")

    if best_map is None or method not in best_map:
        return {"method": method, "status": "missing_best_mc"}

    best_mc = int(best_map[method]["mc"])

    # ä¿è¯ Step09 ä¸ä¼šåµŒå…¥è€å›¾ï¼šå¿…è¦æ—¶å…ˆåˆ·æ–°å¯è§†åŒ–
    if refresh_viz:
        _maybe_refresh_step09(base_dir, method, reference_mtime=refresh_reference_mtime)

    # ä¸ºå½“å‰æ–¹æ³•æ„å»ºâ€œä¸»é¢˜çŸ­è¯­å”¯ä¸€æ˜ å°„â€ï¼ˆåŒä¸€æŠ¥å‘Šå†…ä¸å…è®¸ä¸åŒ Topic æ˜¾ç¤ºç›¸åŒçŸ­è¯­ï¼‰
    topic_phrase_map: Dict[int, str] = {}
    try:
        files = _pick_best_files(base_dir, method, best_mc)
        ti = files.get("topic_info")
        if ti and Path(ti).exists():
            dft = pd.read_csv(Path(ti))
            topic_phrase_map = _build_unique_topic_phrase_map(dft)
    except Exception:
        topic_phrase_map = {}

    out_dir = base_dir / "10_report" / method.upper()
    out_dir.mkdir(parents=True, exist_ok=True)

    md: List[str] = []
    md.append(f"# {get_project_name()} ä¸»é¢˜å»ºæ¨¡ç ”ç©¶æŠ¥å‘Šï¼ˆ{method.upper()}ï¼‰\n")
    md.append(f"- Project Prefix: {PROJECT_PREFIX}")
    md.append(f"- Search Keyword: {SEARCH_KEYWORD}")
    md.append(f"- ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")

    # æ–¹æ³•è®ºç« èŠ‚ï¼ˆæ€»è§ˆ + å…¬å¼/ç¬¦å·/å¤ç°æ€§ï¼›ä¸åœ¨è¿™é‡Œæ”¾â€œæœ¬æ–¹æ³•ç»“æœâ€ï¼Œé¿å…æ‰“ä¹±æ­¥éª¤é¡ºåºï¼‰
    md.extend(_methodology_section(base_dir=base_dir, method=method, best_mc=best_mc))

    # Step04 log
    fl = _read_filter_log(base_dir)
    md.append("## Step 04 ç±»å‹è¿‡æ»¤æ‘˜è¦\n")
    if fl:
        md.append("```text")
        md.append(fl.strip()[:4000])
        md.append("```\n")
    else:
        md.append("- æœªæ‰¾åˆ°è¿‡æ»¤æ—¥å¿—ï¼ˆå¯å¿½ç•¥ï¼‰\n")

    md.extend(_method_stopword_section(method, sw_manifest))
    md.extend(_method_denoise_section(method, denoise_manifest))
    md.extend(_method_topic_model_section(base_dir, method, best_mc, topic_phrase_map=topic_phrase_map))

    # Step08 sectionï¼ˆæ”¾åœ¨ Step07 ä¹‹åï¼Œä¿è¯é¡ºåºï¼›ä»…æ­¤å¤„ç»™å‡ºæœ¬æ–¹æ³•é€‰æ‹©ç»“æœï¼‰
    md.append("## Step 08ï¼šmc é€‰æ‹©ç»“æœï¼ˆæœ¬æ–¹æ³•ï¼‰\n")
    md.append(f"- selected best_mc = {best_mc}")
    md.append(f"- C_v = {best_map[method].get('cv')}")
    if best_map[method].get("noise_ratio") is not None:
        md.append(f"- noise = {best_map[method].get('noise_ratio')}")
    if best_map[method].get("selection_note"):
        md.append(f"- selection_note = {best_map[method].get('selection_note')}")

    # å¦‚æœå­˜åœ¨ Step08 å…¨é‡è®°å½•ï¼ˆcv_scores_full.jsonï¼‰ï¼Œè¡¥å……å¯è¡Œé›†/å¸•ç´¯æ‰˜é›†/è§„åˆ™
    if step08_ctx and isinstance(step08_ctx, dict):
        scores_full = step08_ctx.get("scores_full") or {}
        methods_blob = (scores_full.get("methods") or {}) if isinstance(scores_full, dict) else {}
        mrec = methods_blob.get(method) if isinstance(methods_blob, dict) else None
        if isinstance(mrec, dict):
            sel = mrec.get("selection_details")
            md.append(f"- evaluated_mcs = {mrec.get('evaluated_mcs','-')}")
            if isinstance(sel, dict) and sel.get("noise_ref") is not None:
                try:
                    md.append(f"- noise_ref r = {float(sel.get('noise_ref')):.2%} ({sel.get('noise_ref_label','-')})")
                except Exception:
                    md.append(f"- noise_ref r = {sel.get('noise_ref')} ({sel.get('noise_ref_label','-')})")
            if isinstance(sel, dict) and sel.get("feasible_mcs"):
                md.append(f"- feasible_mcs F(r) = {sel.get('feasible_mcs')}")
            if isinstance(sel, dict) and sel.get("pareto_mcs"):
                md.append(f"- pareto_mcs = {sel.get('pareto_mcs')}")
            if isinstance(sel, dict) and sel.get("rule"):
                md.append(f"- decision_rule = {sel.get('rule')}")
    md.append("")

    # Step09 å¯è§†åŒ–ï¼ˆæ”¾åœ¨ Step08 ä¹‹åï¼‰
    md.extend(_method_viz_section(base_dir, method))

    # â€”â€” ç»“æœ/åˆ†æç« èŠ‚ï¼ˆæ”¾åˆ°æ­¥éª¤ä¹‹åï¼Œç¬¦åˆâ€œå…ˆæ–¹æ³•åç»“æœâ€çš„å†™ä½œä¹ æƒ¯ï¼‰â€”â€”
    md.extend(_method_top_topics_section(base_dir, method, best_mc, topic_phrase_map=topic_phrase_map))
    md.extend(_method_frontier_metrics_section(base_dir, method, best_mc, topic_phrase_map=topic_phrase_map))
    md.extend(_key_findings_section(base_dir, method, best_mc, topic_phrase_map=topic_phrase_map))

    md.extend(_citations_section())

    md_content = "\n".join(md)
    
    # ä¿å­˜ Markdown
    out_file = out_dir / f"{PROJECT_PREFIX}_{method}_report.md"
    out_file.write_text(md_content, encoding="utf-8")
    
    # ä¿å­˜ HTML
    html_file = out_dir / f"{PROJECT_PREFIX}_{method}_report.html"
    html_content = _convert_md_to_html(md_content, f"{get_project_name()} ä¸»é¢˜å»ºæ¨¡ç ”ç©¶æŠ¥å‘Šï¼ˆ{method.upper()}ï¼‰")
    html_file.write_text(html_content, encoding="utf-8")

    return {"method": method, "status": "ok", "report": str(out_file), "html": str(html_file), "best_mc": best_mc}


def _ensure_pandoc(auto_download: bool) -> None:
    if not HAS_PYPANDOC:
        raise RuntimeError("missing_pypandoc")
    try:
        _ = pypandoc.get_pandoc_version()
        return
    except OSError:
        if not auto_download:
            raise
    pypandoc.download_pandoc()
    _ = pypandoc.get_pandoc_version()


def _convert_md_file_to_docx(md_path: Path, docx_path: Path, *, auto_download_pandoc: bool) -> None:
    """Convert a Markdown file to DOCX using pandoc.

    Notes:
    - pandoc resolves relative image paths based on the input file location.
    - Uses common markdown extensions and tex_math_dollars for $...$ / $$...$$.
    """
    _ensure_pandoc(auto_download_pandoc)
    docx_path.parent.mkdir(parents=True, exist_ok=True)
    # Resource path is critical on Windows: pandoc resolves images relative to its resource path
    # (often the current working directory), not necessarily the markdown file directory.
    # We include both the markdown directory and the repo root to make paths like
    # ../../09_visualization/... resolvable.
    resource_path = ";".join([str(md_path.parent), str(Path(__file__).resolve().parent)])

    extra_args = [
        "--quiet",
        "--from",
        "markdown+pipe_tables+grid_tables+fenced_code_blocks+backtick_code_blocks+tex_math_dollars",
        "--resource-path",
        resource_path,
    ]

    # pandoc åœ¨ Windows ä¸Šçš„ stdout/stderr å¯èƒ½ä½¿ç”¨æœ¬åœ°ä»£ç é¡µï¼ˆå¦‚ cp936ï¼‰ï¼Œ
    # pypandoc é»˜è®¤æŒ‰ utf-8 è§£ç ä¼šæŠ›å‡º "Pandoc output was not utf-8."ã€‚
    # è¿™é‡Œä½¿ç”¨ç³»ç»Ÿé¦–é€‰ç¼–ç è§£ç ï¼Œä»…å½±å“æ—¥å¿—è§£ç ï¼Œä¸å½±å“ç”Ÿæˆçš„ docxã€‚
    pandoc_encoding = locale.getpreferredencoding(False) or "utf-8"
    pypandoc.convert_file(
        str(md_path),
        to="docx",
        format="md",
        outputfile=str(docx_path),
        extra_args=extra_args,
        encoding=pandoc_encoding,
    )


def main() -> int:
    parser = argparse.ArgumentParser(description="Step10: æŠ¥å‘Šç”Ÿæˆ")
    parser.add_argument(
        "--base_dir",
        default=None,
        help="å·¥ä½œç›®å½•ï¼ˆåŒ…å« 07_topic_models/08_model_selection/10_report ç­‰ï¼‰ã€‚é»˜è®¤ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ã€‚",
    )
    parser.add_argument("--only", help="åªè·‘æŒ‡å®šæ–¹æ³•ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼šbaseline/A/B/C/AB/ABCï¼‰")
    parser.add_argument("--docx", action="store_true", help="åŒæ—¶ç”Ÿæˆ Word æ–‡æ¡£ï¼ˆ.docxï¼Œéœ€ pandocï¼‰")
    parser.add_argument(
        "--no-refresh-viz",
        action="store_true",
        help="ä¸è‡ªåŠ¨åˆ·æ–° Step09 å¯è§†åŒ–ï¼ˆé»˜è®¤è‹¥æ£€æµ‹åˆ°ç¼ºå¤±/è¿‡æœŸä¼šè‡ªåŠ¨é‡è·‘ Step09ï¼Œé¿å…åµŒå…¥è€å›¾ï¼‰",
    )
    parser.add_argument(
        "--no-download-pandoc",
        action="store_true",
        help="pandoc ç¼ºå¤±æ—¶ä¸è‡ªåŠ¨ä¸‹è½½ï¼ˆé»˜è®¤ä¼šè‡ªåŠ¨ä¸‹è½½ pandocï¼‰",
    )
    args = parser.parse_args()

    def _resolve_base_dir() -> Path:
        if args.base_dir:
            return Path(args.base_dir).resolve()
        # è‹¥å­˜åœ¨ä¸»æµç¨‹æŒ‡é’ˆï¼Œåˆ™é»˜è®¤ä½¿ç”¨å®ƒ
        ptr = Path(__file__).resolve().parent / "reproducible_pipeline" / "MAIN_WORKDIR.txt"
        if ptr.exists():
            try:
                p = ptr.read_text(encoding="utf-8", errors="replace").strip().strip('"')
                if p:
                    cand = Path(p).expanduser()
                    if cand.exists():
                        return cand.resolve()
            except Exception:
                pass
        return Path(__file__).resolve().parent

    base_dir = _resolve_base_dir()
    if args.only:
        only_raw = str(args.only).strip()
        # å…¼å®¹ï¼šç”¨æˆ·ä¹ æƒ¯ç”¨ C æŒ‡ä»£å½“å‰ä¸»æµç¨‹çš„ VPD
        if only_raw.upper() in ("VPD", "C"):
            only_norm = "VPD"
        elif only_raw.lower() == "baseline":
            only_norm = "baseline"
        else:
            only_norm = only_raw.upper() if only_raw.upper() in ("A", "B", "AB", "ABC") else only_raw.lower()

        if only_norm not in ALL_METHODS:
            print(f"--only å‚æ•°æ— æ•ˆ: {only_raw}")
            print("å¯é€‰: baseline/VPDï¼ˆå…¼å®¹åˆ«å Cï¼‰")
            return 2
        methods = [only_norm]
    else:
        methods = ALL_METHODS

    print("=" * 80)
    print(f"Step 10 æŠ¥å‘Šç”Ÿæˆ - {get_project_name()} ({PROJECT_PREFIX})")
    print("=" * 80)

    step08_ctx = _load_step08_context(base_dir)
    refresh_ref_mtime = float(max(step08_ctx.get("best_mtime", 0.0), step08_ctx.get("scores_mtime", 0.0)))

    ok_any = False
    for m in methods:
        print(f"\nâ†’ ç”Ÿæˆ {m} æŠ¥å‘Š...", end="", flush=True)
        r = build_report_for_method(
            base_dir,
            m,
            step08_ctx=step08_ctx,
            refresh_viz=not args.no_refresh_viz,
            refresh_reference_mtime=refresh_ref_mtime,
        )
        if r.get("status") != "ok":
            print(f"âœ— ({r.get('status')})")
            continue
        print("âœ“")
        print(f"  best_mc={r.get('best_mc')}  MD: {r.get('report')}")
        print(f"               HTML: {r.get('html')}")

        if args.docx:
            md_path = Path(r.get("report"))
            docx_path = md_path.with_suffix(".docx")
            try:
                _convert_md_file_to_docx(md_path, docx_path, auto_download_pandoc=not args.no_download_pandoc)
                print(f"               DOCX: {docx_path.as_posix()}")
            except Exception as exc:
                # ä¸è®© docx å¤±è´¥å½±å“ä¸»æµç¨‹ï¼ˆMD/HTML å·²å®Œæˆï¼‰
                reason = str(exc)
                if isinstance(exc, RuntimeError) and str(exc) == "missing_pypandoc":
                    reason = "ç¼ºå°‘ä¾èµ– pypandocï¼ˆpip install pypandocï¼‰"
                print(f"               DOCX: âœ— ({reason})")

        ok_any = True

    print("\n" + "=" * 80)
    print("Step 10 å®Œæˆ")
    print("è¾“å‡ºç›®å½•: 10_report/")
    print("=" * 80)

    return 0 if ok_any else 1


if __name__ == "__main__":
    raise SystemExit(main())
