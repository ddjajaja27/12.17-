#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_ablation_experiments.py

æ‰§è¡Œæ¶ˆèå®éªŒï¼š4 ä¸ªç‰ˆæœ¬çš„æŠ•å½±ï¼ˆæ— æŠ•å½±/M+S/M+S+B/å…¨éƒ¨ï¼‰
å¯¹æ¯ä¸ªç‰ˆæœ¬è·‘ BERTopicï¼Œå¯¹æ¯” C_vã€ä¸»é¢˜è¯è´¨é‡ã€æ‹“æ‰‘ç»“æ„ç¨³å®šæ€§
"""

from __future__ import annotations

import json
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
import sys

sys.path.insert(0, str(Path(__file__).resolve().parent))

try:
    from ablation_experiment_config import ABLATION_CONFIGS, NoiseWordGroups
    from config import PROJECT_PREFIX
except ImportError as e:
    print(f"ç¼ºå°‘é…ç½®æ¨¡å—: {e}")
    sys.exit(1)


def load_raw_embeddings() -> Tuple[np.ndarray, np.ndarray]:
    """åŠ è½½åŸå§‹èåˆå‘é‡"""
    root = Path(__file__).resolve().parent
    fused_path = root / "05_stopwords" / "Experiment_C_Vector" / "data" / "c_step1_fused_vectors.npz"
    
    if not fused_path.exists():
        raise FileNotFoundError(f"èåˆå‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {fused_path}")
    
    data = np.load(fused_path, allow_pickle=True)
    return data["pmids"], data["fused_vectors"]


def build_noise_prototype(embedding_model, noise_words: List[str]) -> np.ndarray:
    """æ„å»ºå™ªå£°åŸå‹å‘é‡"""
    if not noise_words:
        return None
    
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        raise RuntimeError("ç¼ºå°‘ sentence_transformersï¼Œè¯·å…ˆ pip install sentence-transformers")
    
    if embedding_model is None:
        embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
    
    # ç¼–ç å™ªå£°è¯
    noise_embeddings = embedding_model.encode(
        noise_words,
        normalize_embeddings=True,
        show_progress_bar=False,
        convert_to_numpy=True
    )
    
    # å¹³å‡ + å½’ä¸€åŒ–
    noise_prototype = np.mean(noise_embeddings, axis=0)
    noise_prototype = noise_prototype / (np.linalg.norm(noise_prototype) + 1e-10)
    
    return noise_prototype


def project_vectors(vectors: np.ndarray, noise_direction: np.ndarray) -> np.ndarray:
    """æ­£äº¤æŠ•å½±ï¼ˆå®Œå…¨ç§»é™¤å™ªå£°æ–¹å‘ï¼‰"""
    if noise_direction is None:
        return vectors.copy()
    
    projection_lengths = vectors @ noise_direction
    projection_vectors = np.outer(projection_lengths, noise_direction)
    clean_vectors = vectors - projection_vectors
    
    # é‡æ–°å½’ä¸€åŒ–
    norms = np.linalg.norm(clean_vectors, axis=1, keepdims=True)
    clean_vectors = clean_vectors / (norms + 1e-10)
    
    return clean_vectors


def run_single_ablation(
    config_name: str,
    config: Dict[str, Any],
    pmids: np.ndarray,
    fused_vectors: np.ndarray,
    embedding_model=None,
) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªæ¶ˆèé…ç½®"""
    print(f"\n{'='*70}")
    print(f"ğŸ§ª è¿è¡Œ: {config['name']}")
    print(f"   å™ªå£°è¯æ•°: {len(config['noise_words'])}")
    print(f"{'='*70}")
    
    # æ„å»ºå™ªå£°æ–¹å‘
    if config['noise_words']:
        noise_direction = build_noise_prototype(embedding_model, config['noise_words'])
        print(f"å™ªå£°åŸå‹èŒƒæ•°: {np.linalg.norm(noise_direction):.4f}")
    else:
        noise_direction = None
        print("ï¼ˆæ— æŠ•å½±ï¼Œbaselineï¼‰")
    
    # æŠ•å½±
    clean_vectors = project_vectors(fused_vectors, noise_direction)
    
    # è®¡ç®—æŠ•å½±æ•ˆæœ
    if noise_direction is not None:
        original_sim = np.mean(fused_vectors @ noise_direction)
        clean_sim = np.mean(clean_vectors @ noise_direction)
        reduction = (1 - clean_sim / original_sim) * 100 if original_sim != 0 else 0
        print(f"å™ªå£°ç›¸ä¼¼åº¦: {original_sim:.4f} â†’ {clean_sim:.6f} (å‡å°‘ {reduction:.1f}%)")
    
    # ä¿å­˜æŠ•å½±åçš„å‘é‡
    output_dir = Path(__file__).resolve().parent / "ablation_outputs" / config_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    vector_file = output_dir / f"embeddings_{config_name}.npz"
    np.savez_compressed(
        vector_file,
        pmids=pmids,
        embeddings=clean_vectors.astype(np.float32),
    )
    print(f"âœ“ å‘é‡å·²ä¿å­˜: {vector_file}")
    
    return {
        "config_name": config_name,
        "config_name_display": config['name'],
        "noise_words_count": len(config['noise_words']),
        "vector_file": str(vector_file),
        "original_noise_sim": float(np.mean(fused_vectors @ noise_direction)) if noise_direction is not None else None,
        "clean_noise_sim": float(np.mean(clean_vectors @ noise_direction)) if noise_direction is not None else None,
    }


def main():
    """ä¸»æµç¨‹"""
    print("=" * 70)
    print("ğŸ§ª VPD æ¶ˆèå®éªŒ - å‘é‡æŠ•å½±é˜¶æ®µ")
    print("=" * 70)
    
    # åŠ è½½åŸå§‹å‘é‡
    print("\nåŠ è½½åŸå§‹èåˆå‘é‡...")
    pmids, fused_vectors = load_raw_embeddings()
    print(f"  æ–‡æ¡£æ•°: {len(pmids)}")
    print(f"  ç»´åº¦: {fused_vectors.shape[1]}")
    
    # åŠ è½½ embedding æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼Œåç»­å¤ç”¨ï¼‰
    print("\nåŠ è½½ Sentence Transformer æ¨¡å‹...")
    try:
        from sentence_transformers import SentenceTransformer
        embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        print("  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"  âœ— åŠ è½½å¤±è´¥: {e}")
        embedding_model = None
    
    # è¿è¡Œ 4 ä¸ªæ¶ˆèé…ç½®
    results = []
    for config_name, config in ABLATION_CONFIGS.items():
        result = run_single_ablation(
            config_name,
            config,
            pmids,
            fused_vectors,
            embedding_model
        )
        results.append(result)
    
    # ä¿å­˜ç»“æœæ±‡æ€»
    output_dir = Path(__file__).resolve().parent / "ablation_outputs"
    summary_file = output_dir / "ablation_summary.json"
    
    summary = {
        "timestamp": str(Path(__file__).resolve().parent),
        "configs": ABLATION_CONFIGS,
        "results": results,
        "total_docs": len(pmids),
        "embedding_dim": fused_vectors.shape[1],
    }
    
    with open(summary_file, "w", encoding="utf-8") as f:
        # ç®€åŒ– ABLATION_CONFIGS ä»¥ä¾¿ JSON åºåˆ—åŒ–
        summary["configs"] = {
            k: {
                "name": v["name"],
                "noise_words_count": len(v["noise_words"]),
                "description": v["description"]
            }
            for k, v in ABLATION_CONFIGS.items()
        }
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ æ¶ˆèå®éªŒæ±‡æ€»å·²ä¿å­˜: {summary_file}")
    
    # æ‰“å°å…³é”®é—®é¢˜æ¸…å•
    print("\n" + "=" * 70)
    print("ğŸ“‹ ä¸‹ä¸€æ­¥ï¼šç”¨è¿™äº›å‘é‡è·‘ BERTopic")
    print("=" * 70)
    print("\nã€4 ä¸ªç‰ˆæœ¬çš„å‘é‡æ–‡ä»¶ã€‘")
    for result in results:
        print(f"  {result['config_name']}: {result['vector_file']}")
    
    print("\nã€å…³é”®å¯¹æ¯”ç»´åº¦ã€‘")
    print("  1. C_v ä¸€è‡´æ€§ (åŸæœ‰æŒ‡æ ‡)")
    print("  2. ä¸»é¢˜è¯è´¨é‡ (æ–°å¢ï¼šæ˜¯å¦æ›´ç”Ÿç‰©å­¦æ„ä¹‰)")
    print("  3. Silhouette coefficient (æ–°å¢ï¼šç°‡çš„ç´§å‡‘æ€§)")
    print("  4. kNN mixing (æ–°å¢ï¼šè·¨æ¿å—çš„é‚»å±…æ¯”ä¾‹)")
    print("  5. æ‹“æ‰‘ç¨³å®šæ€§ (æ–°å¢ï¼šä¸åŒ seed ä¸‹ç»“æ„ä¸€è‡´æ€§)")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
